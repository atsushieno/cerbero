From 40ac33ae70733cc0074f8784c922e7020e6c2ef7 Mon Sep 17 00:00:00 2001
From: =?UTF-8?q?Tim-Philipp=20M=C3=BCller?= <tim@centricular.com>
Date: Fri, 16 Aug 2019 13:08:48 +0100
Subject: [PATCH 5/5] asm: rename some assembly functions to not conflict with
 openssl

Otherwise including gnutls and openssl in the same library from static
components would produce duplicate symbols and fail to link.

Done using:
for f in `git grep -h '^.globl' lib/accelerated/ | sed -e 's/^\.globl\s*//' | env LC_COLLATE=C sort` ; do sed -i -e s/\\b$f\\b/_gnutls_$f/g `git ls-files lib/accelerated`; done

And then:
sed -i -e 's/_gnutls__/__gnutls_/g' lib/accelerated/x86/coff/*.s
---
 lib/accelerated/aarch64/aes-aarch64.h         |  10 +-
 lib/accelerated/aarch64/aes-cbc-aarch64.c     |   8 +-
 lib/accelerated/aarch64/aes-ccm-aarch64.c     |   4 +-
 lib/accelerated/aarch64/aes-gcm-aarch64.c     |  24 ++--
 lib/accelerated/aarch64/elf/aes-aarch64.s     |  48 +++----
 lib/accelerated/aarch64/elf/ghash-aarch64.s   |  24 ++--
 lib/accelerated/aarch64/elf/sha1-armv8.s      |   8 +-
 lib/accelerated/aarch64/elf/sha256-armv8.s    |   8 +-
 lib/accelerated/aarch64/elf/sha512-armv8.s    |   8 +-
 lib/accelerated/aarch64/sha-aarch64.c         |  12 +-
 lib/accelerated/x86/aes-cbc-x86-aesni.c       |   8 +-
 lib/accelerated/x86/aes-cbc-x86-ssse3.c       |   8 +-
 lib/accelerated/x86/aes-ccm-x86-aesni.c       |   4 +-
 lib/accelerated/x86/aes-gcm-padlock.c         |   2 +-
 lib/accelerated/x86/aes-gcm-x86-aesni.c       |   6 +-
 lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c  |  30 ++--
 lib/accelerated/x86/aes-gcm-x86-pclmul.c      |  26 ++--
 lib/accelerated/x86/aes-gcm-x86-ssse3.c       |   6 +-
 lib/accelerated/x86/aes-padlock.c             |   6 +-
 lib/accelerated/x86/aes-padlock.h             |   8 +-
 lib/accelerated/x86/aes-x86.h                 |  24 ++--
 lib/accelerated/x86/coff/aes-ssse3-x86.s      |  30 ++--
 lib/accelerated/x86/coff/aes-ssse3-x86_64.s   |  30 ++--
 lib/accelerated/x86/coff/aesni-gcm-x86_64.s   |  12 +-
 lib/accelerated/x86/coff/aesni-x86.s          |  78 +++++------
 lib/accelerated/x86/coff/aesni-x86_64.s       |  82 +++++------
 lib/accelerated/x86/coff/e_padlock-x86.s      |  96 ++++++-------
 lib/accelerated/x86/coff/e_padlock-x86_64.s   |  96 ++++++-------
 lib/accelerated/x86/coff/ghash-x86_64.s       |  48 +++----
 lib/accelerated/x86/coff/sha1-ssse3-x86.s     |   6 +-
 lib/accelerated/x86/coff/sha1-ssse3-x86_64.s  |   6 +-
 lib/accelerated/x86/coff/sha256-ssse3-x86.s   |   6 +-
 .../x86/coff/sha256-ssse3-x86_64.s            |   6 +-
 lib/accelerated/x86/coff/sha512-ssse3-x86.s   |   6 +-
 .../x86/coff/sha512-ssse3-x86_64.s            |   6 +-
 lib/accelerated/x86/elf/aes-ssse3-x86.s       |  40 +++---
 lib/accelerated/x86/elf/aes-ssse3-x86_64.s    |  40 +++---
 lib/accelerated/x86/elf/aesni-gcm-x86_64.s    |  16 +--
 lib/accelerated/x86/elf/aesni-x86.s           | 114 ++++++++--------
 lib/accelerated/x86/elf/aesni-x86_64.s        | 104 +++++++-------
 lib/accelerated/x86/elf/e_padlock-x86.s       | 128 +++++++++---------
 lib/accelerated/x86/elf/e_padlock-x86_64.s    | 128 +++++++++---------
 lib/accelerated/x86/elf/ghash-x86_64.s        |  64 ++++-----
 lib/accelerated/x86/elf/sha1-ssse3-x86.s      |   8 +-
 lib/accelerated/x86/elf/sha1-ssse3-x86_64.s   |   8 +-
 lib/accelerated/x86/elf/sha256-ssse3-x86.s    |   8 +-
 lib/accelerated/x86/elf/sha256-ssse3-x86_64.s |   8 +-
 lib/accelerated/x86/elf/sha512-ssse3-x86.s    |   8 +-
 lib/accelerated/x86/elf/sha512-ssse3-x86_64.s |   8 +-
 lib/accelerated/x86/macosx/aes-ssse3-x86.s    |  20 +--
 lib/accelerated/x86/macosx/aes-ssse3-x86_64.s |  20 +--
 lib/accelerated/x86/macosx/aesni-gcm-x86_64.s |   8 +-
 lib/accelerated/x86/macosx/aesni-x86.s        |  52 +++----
 lib/accelerated/x86/macosx/aesni-x86_64.s     |  52 +++----
 lib/accelerated/x86/macosx/e_padlock-x86.s    |  64 ++++-----
 lib/accelerated/x86/macosx/e_padlock-x86_64.s |  64 ++++-----
 lib/accelerated/x86/macosx/ghash-x86_64.s     |  32 ++---
 lib/accelerated/x86/macosx/sha1-ssse3-x86.s   |   4 +-
 .../x86/macosx/sha1-ssse3-x86_64.s            |   4 +-
 lib/accelerated/x86/macosx/sha256-ssse3-x86.s |   4 +-
 .../x86/macosx/sha256-ssse3-x86_64.s          |   4 +-
 lib/accelerated/x86/macosx/sha512-ssse3-x86.s |   4 +-
 .../x86/macosx/sha512-ssse3-x86_64.s          |   4 +-
 lib/accelerated/x86/sha-padlock.c             |  10 +-
 lib/accelerated/x86/sha-padlock.h             |  10 +-
 lib/accelerated/x86/sha-x86-ssse3.c           |  12 +-
 lib/accelerated/x86/x86-common.c              |   6 +-
 67 files changed, 923 insertions(+), 923 deletions(-)

diff --git a/lib/accelerated/aarch64/aes-aarch64.h b/lib/accelerated/aarch64/aes-aarch64.h
index 692d862..5638b9b 100644
--- a/lib/accelerated/aarch64/aes-aarch64.h
+++ b/lib/accelerated/aarch64/aes-aarch64.h
@@ -20,12 +20,12 @@ typedef struct {
 	if (s != 16 && s != 24 && s != 32) \
 		return GNUTLS_E_INVALID_REQUEST
 
-int aes_v8_set_encrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);  
-int aes_v8_set_decrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);
-void aes_v8_cbc_encrypt(const unsigned char *in, unsigned char *out,
+int _gnutls_aes_v8_set_encrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);  
+int _gnutls_aes_v8_set_decrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);
+void _gnutls_aes_v8_cbc_encrypt(const unsigned char *in, unsigned char *out,
                        size_t length, const AES_KEY *key, unsigned char *ivec, int enc);
-void aes_v8_encrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
-void aes_v8_decrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
+void _gnutls_aes_v8_encrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
+void _gnutls_aes_v8_decrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
 
 extern const gnutls_crypto_cipher_st _gnutls_aes_gcm_aarch64;
 extern const gnutls_crypto_cipher_st _gnutls_aes_cbc_aarch64;
diff --git a/lib/accelerated/aarch64/aes-cbc-aarch64.c b/lib/accelerated/aarch64/aes-cbc-aarch64.c
index 68434f3..10b541c 100644
--- a/lib/accelerated/aarch64/aes-cbc-aarch64.c
+++ b/lib/accelerated/aarch64/aes-cbc-aarch64.c
@@ -69,11 +69,11 @@ aes_aarch64_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 
 	if (ctx->enc)
 		ret =
-		    aes_v8_set_encrypt_key(userkey, keysize * 8,
+		    _gnutls_aes_v8_set_encrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 	else
 		ret =
-		    aes_v8_set_decrypt_key(userkey, keysize * 8,
+		    _gnutls_aes_v8_set_decrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 
 	if (ret != 0)
@@ -91,7 +91,7 @@ aes_aarch64_encrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	aes_v8_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_aes_v8_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 1);
 	return 0;
 }
@@ -105,7 +105,7 @@ aes_aarch64_decrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	aes_v8_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_aes_v8_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 0);
 
 	return 0;
diff --git a/lib/accelerated/aarch64/aes-ccm-aarch64.c b/lib/accelerated/aarch64/aes-ccm-aarch64.c
index 5de7ab0..7400334 100644
--- a/lib/accelerated/aarch64/aes-ccm-aarch64.c
+++ b/lib/accelerated/aarch64/aes-ccm-aarch64.c
@@ -51,7 +51,7 @@ static void aarch64_aes_encrypt(const void *_ctx,
 	unsigned i;
 
 	for (i=0;i<length;i+=16) {
-		aes_v8_encrypt(src, dst, ctx);
+		_gnutls_aes_v8_encrypt(src, dst, ctx);
 		src+=16;
 		dst+=16;
 	}
@@ -84,7 +84,7 @@ aes_ccm_cipher_setkey(void *_ctx, const void *key, size_t length)
 
 	CHECK_AES_KEYSIZE(length);
 
-	aes_v8_set_encrypt_key(key, length*8, &ctx->key);
+	_gnutls_aes_v8_set_encrypt_key(key, length*8, &ctx->key);
 
 	return 0;
 }
diff --git a/lib/accelerated/aarch64/aes-gcm-aarch64.c b/lib/accelerated/aarch64/aes-gcm-aarch64.c
index c88fe97..15458d3 100644
--- a/lib/accelerated/aarch64/aes-gcm-aarch64.c
+++ b/lib/accelerated/aarch64/aes-gcm-aarch64.c
@@ -64,10 +64,10 @@ struct aes_gcm_ctx {
 	unsigned auth_finished;
 };
 
-void gcm_init_v8(u128 Htable[16], const uint64_t Xi[2]);
-void gcm_ghash_v8(uint64_t Xi[2], const u128 Htable[16],
+void _gnutls_gcm_init_v8(u128 Htable[16], const uint64_t Xi[2]);
+void _gnutls_gcm_ghash_v8(uint64_t Xi[2], const u128 Htable[16],
 		     const uint8_t * inp, size_t len);
-void gcm_gmult_v8(uint64_t Xi[2], const u128 Htable[16]);
+void _gnutls_gcm_gmult_v8(uint64_t Xi[2], const u128 Htable[16]);
 
 static void aes_gcm_deinit(void *_ctx)
 {
@@ -104,17 +104,17 @@ aes_gcm_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 	CHECK_AES_KEYSIZE(keysize);
 
 	ret =
-	    aes_v8_set_encrypt_key(userkey, keysize * 8,
+	    _gnutls_aes_v8_set_encrypt_key(userkey, keysize * 8,
 				  ALIGN16(&ctx->expanded_key));
 	if (ret != 0)
 		return gnutls_assert_val(GNUTLS_E_ENCRYPTION_FAILED);
 
-	aes_v8_encrypt(ctx->gcm.H.c, ctx->gcm.H.c, ALIGN16(&ctx->expanded_key));
+	_gnutls_aes_v8_encrypt(ctx->gcm.H.c, ctx->gcm.H.c, ALIGN16(&ctx->expanded_key));
 
 	ctx->gcm.H.u[0] = bswap_64(ctx->gcm.H.u[0]);
 	ctx->gcm.H.u[1] = bswap_64(ctx->gcm.H.u[1]);
 
-	gcm_init_v8(ctx->gcm.Htable, ctx->gcm.H.u);
+	_gnutls_gcm_init_v8(ctx->gcm.Htable, ctx->gcm.H.u);
 
 	return 0;
 }
@@ -135,7 +135,7 @@ static int aes_gcm_setiv(void *_ctx, const void *iv, size_t iv_size)
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 2] = 0;
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 1;
 
-	aes_v8_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
+	_gnutls_aes_v8_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
 			ALIGN16(&ctx->expanded_key));
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 2;
 	ctx->finished = 0;
@@ -150,12 +150,12 @@ gcm_ghash(struct aes_gcm_ctx *ctx, const uint8_t * src, size_t src_size)
 	size_t aligned_size = src_size - rest;
 
 	if (aligned_size > 0)
-		gcm_ghash_v8(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
+		_gnutls_gcm_ghash_v8(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
 				aligned_size);
 
 	if (rest > 0) {
 		memxor(ctx->gcm.Xi.c, src + aligned_size, rest);
-		gcm_gmult_v8(ctx->gcm.Xi.u, ctx->gcm.Htable);
+		_gnutls_gcm_gmult_v8(ctx->gcm.Xi.u, ctx->gcm.Htable);
 	}
 }
 
@@ -171,7 +171,7 @@ ctr32_encrypt_blocks_inplace(const unsigned char *in, unsigned char *out,
 	memcpy(ctr, ivec, 16);
 
 	for (i=0;i<blocks;i++) {
-		aes_v8_encrypt(ctr, tmp, key);
+		_gnutls_aes_v8_encrypt(ctr, tmp, key);
 		memxor3(out, tmp, in, 16);
 
 		out += 16;
@@ -194,7 +194,7 @@ ctr32_encrypt_blocks(const unsigned char *in, unsigned char *out,
 	memcpy(ctr, ivec, 16);
 
 	for (i=0;i<blocks;i++) {
-		aes_v8_encrypt(ctr, out, key);
+		_gnutls_aes_v8_encrypt(ctr, out, key);
 		memxor(out, in, 16);
 
 		out += 16;
@@ -318,7 +318,7 @@ static void aes_gcm_tag(void *_ctx, void *tag, size_t tagsize)
 	_gnutls_write_uint64(alen, buffer);
 	_gnutls_write_uint64(clen, &buffer[8]);
 
-	gcm_ghash_v8(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
+	_gnutls_gcm_ghash_v8(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
 			GCM_BLOCK_SIZE);
 
 	ctx->gcm.Xi.u[0] ^= ctx->gcm.EK0.u[0];
diff --git a/lib/accelerated/aarch64/elf/aes-aarch64.s b/lib/accelerated/aarch64/elf/aes-aarch64.s
index ab227a8..7db8138 100644
--- a/lib/accelerated/aarch64/elf/aes-aarch64.s
+++ b/lib/accelerated/aarch64/elf/aes-aarch64.s
@@ -53,10 +53,10 @@
 .long 0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d,0x0c0f0e0d
 .long 0x1b,0x1b,0x1b,0x1b
 
-.globl aes_v8_set_encrypt_key
-.type aes_v8_set_encrypt_key,%function
+.globl _gnutls_aes_v8_set_encrypt_key
+.type _gnutls_aes_v8_set_encrypt_key,%function
 .align 5
-aes_v8_set_encrypt_key:
+_gnutls_aes_v8_set_encrypt_key:
 .Lenc_key:
  stp x29,x30,[sp,#-16]!
  add x29,sp,#0
@@ -218,12 +218,12 @@ aes_v8_set_encrypt_key:
  mov x0,x3
  ldr x29,[sp],#16
  ret
-.size aes_v8_set_encrypt_key,.-aes_v8_set_encrypt_key
+.size _gnutls_aes_v8_set_encrypt_key,.-_gnutls_aes_v8_set_encrypt_key
 
-.globl aes_v8_set_decrypt_key
-.type aes_v8_set_decrypt_key,%function
+.globl _gnutls_aes_v8_set_decrypt_key
+.type _gnutls_aes_v8_set_decrypt_key,%function
 .align 5
-aes_v8_set_decrypt_key:
+_gnutls_aes_v8_set_decrypt_key:
 .inst 0xd503233f
  stp x29,x30,[sp,#-16]!
  add x29,sp,#0
@@ -260,11 +260,11 @@ aes_v8_set_decrypt_key:
  ldp x29,x30,[sp],#16
 .inst 0xd50323bf
  ret
-.size aes_v8_set_decrypt_key,.-aes_v8_set_decrypt_key
-.globl aes_v8_encrypt
-.type aes_v8_encrypt,%function
+.size _gnutls_aes_v8_set_decrypt_key,.-_gnutls_aes_v8_set_decrypt_key
+.globl _gnutls_aes_v8_encrypt
+.type _gnutls_aes_v8_encrypt,%function
 .align 5
-aes_v8_encrypt:
+_gnutls_aes_v8_encrypt:
  ldr w3,[x2,#240]
  ld1 {v0.4s},[x2],#16
  ld1 {v2.16b},[x0]
@@ -289,11 +289,11 @@ aes_v8_encrypt:
 
  st1 {v2.16b},[x1]
  ret
-.size aes_v8_encrypt,.-aes_v8_encrypt
-.globl aes_v8_decrypt
-.type aes_v8_decrypt,%function
+.size _gnutls_aes_v8_encrypt,.-_gnutls_aes_v8_encrypt
+.globl _gnutls_aes_v8_decrypt
+.type _gnutls_aes_v8_decrypt,%function
 .align 5
-aes_v8_decrypt:
+_gnutls_aes_v8_decrypt:
  ldr w3,[x2,#240]
  ld1 {v0.4s},[x2],#16
  ld1 {v2.16b},[x0]
@@ -318,11 +318,11 @@ aes_v8_decrypt:
 
  st1 {v2.16b},[x1]
  ret
-.size aes_v8_decrypt,.-aes_v8_decrypt
-.globl aes_v8_cbc_encrypt
-.type aes_v8_cbc_encrypt,%function
+.size _gnutls_aes_v8_decrypt,.-_gnutls_aes_v8_decrypt
+.globl _gnutls_aes_v8_cbc_encrypt
+.type _gnutls_aes_v8_cbc_encrypt,%function
 .align 5
-aes_v8_cbc_encrypt:
+_gnutls_aes_v8_cbc_encrypt:
  stp x29,x30,[sp,#-16]!
  add x29,sp,#0
  subs x2,x2,#16
@@ -608,11 +608,11 @@ aes_v8_cbc_encrypt:
 .Lcbc_abort:
  ldr x29,[sp],#16
  ret
-.size aes_v8_cbc_encrypt,.-aes_v8_cbc_encrypt
-.globl aes_v8_ctr32_encrypt_blocks
-.type aes_v8_ctr32_encrypt_blocks,%function
+.size _gnutls_aes_v8_cbc_encrypt,.-_gnutls_aes_v8_cbc_encrypt
+.globl _gnutls_aes_v8_ctr32_encrypt_blocks
+.type _gnutls_aes_v8_ctr32_encrypt_blocks,%function
 .align 5
-aes_v8_ctr32_encrypt_blocks:
+_gnutls_aes_v8_ctr32_encrypt_blocks:
  stp x29,x30,[sp,#-16]!
  add x29,sp,#0
  ldr w5,[x3,#240]
@@ -789,5 +789,5 @@ aes_v8_ctr32_encrypt_blocks:
 .Lctr32_done:
  ldr x29,[sp],#16
  ret
-.size aes_v8_ctr32_encrypt_blocks,.-aes_v8_ctr32_encrypt_blocks
+.size _gnutls_aes_v8_ctr32_encrypt_blocks,.-_gnutls_aes_v8_ctr32_encrypt_blocks
 .section .note.GNU-stack,"",%progbits
diff --git a/lib/accelerated/aarch64/elf/ghash-aarch64.s b/lib/accelerated/aarch64/elf/ghash-aarch64.s
index c301399..61d1d32 100644
--- a/lib/accelerated/aarch64/elf/ghash-aarch64.s
+++ b/lib/accelerated/aarch64/elf/ghash-aarch64.s
@@ -47,10 +47,10 @@
 
 .text
 .arch armv8-a+crypto
-.globl gcm_init_v8
-.type gcm_init_v8,%function
+.globl _gnutls_gcm_init_v8
+.type _gnutls_gcm_init_v8,%function
 .align 4
-gcm_init_v8:
+_gnutls_gcm_init_v8:
  ld1 {v17.2d},[x1]
  movi v19.16b,#0xe1
  shl v19.2d,v19.2d,#57
@@ -136,11 +136,11 @@ gcm_init_v8:
  ext v21.16b,v16.16b,v17.16b,#8
  st1 {v20.2d,v21.2d,v22.2d},[x0]
  ret
-.size gcm_init_v8,.-gcm_init_v8
-.globl gcm_gmult_v8
-.type gcm_gmult_v8,%function
+.size _gnutls_gcm_init_v8,.-_gnutls_gcm_init_v8
+.globl _gnutls_gcm_gmult_v8
+.type _gnutls_gcm_gmult_v8,%function
 .align 4
-gcm_gmult_v8:
+_gnutls_gcm_gmult_v8:
  ld1 {v17.2d},[x0]
  movi v19.16b,#0xe1
  ld1 {v20.2d,v21.2d},[x1]
@@ -177,11 +177,11 @@ gcm_gmult_v8:
  st1 {v0.2d},[x0]
 
  ret
-.size gcm_gmult_v8,.-gcm_gmult_v8
-.globl gcm_ghash_v8
-.type gcm_ghash_v8,%function
+.size _gnutls_gcm_gmult_v8,.-_gnutls_gcm_gmult_v8
+.globl _gnutls_gcm_ghash_v8
+.type _gnutls_gcm_ghash_v8,%function
 .align 4
-gcm_ghash_v8:
+_gnutls_gcm_ghash_v8:
  cmp x3,#64
  b.hs .Lgcm_ghash_v8_4x
  ld1 {v0.2d},[x0]
@@ -302,7 +302,7 @@ gcm_ghash_v8:
  st1 {v0.2d},[x0]
 
  ret
-.size gcm_ghash_v8,.-gcm_ghash_v8
+.size _gnutls_gcm_ghash_v8,.-_gnutls_gcm_ghash_v8
 .type gcm_ghash_v8_4x,%function
 .align 4
 gcm_ghash_v8_4x:
diff --git a/lib/accelerated/aarch64/elf/sha1-armv8.s b/lib/accelerated/aarch64/elf/sha1-armv8.s
index 4b65cf6..2fd9f1d 100644
--- a/lib/accelerated/aarch64/elf/sha1-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha1-armv8.s
@@ -47,10 +47,10 @@
 .text
 
 
-.globl sha1_block_data_order
-.type sha1_block_data_order,%function
+.globl _gnutls_sha1_block_data_order
+.type _gnutls_sha1_block_data_order,%function
 .align 6
-sha1_block_data_order:
+_gnutls_sha1_block_data_order:
 
 
 
@@ -1116,7 +1116,7 @@ sha1_block_data_order:
  ldp x27,x28,[sp,#80]
  ldr x29,[sp],#96
  ret
-.size sha1_block_data_order,.-sha1_block_data_order
+.size _gnutls_sha1_block_data_order,.-_gnutls_sha1_block_data_order
 .type sha1_block_armv8,%function
 .align 6
 sha1_block_armv8:
diff --git a/lib/accelerated/aarch64/elf/sha256-armv8.s b/lib/accelerated/aarch64/elf/sha256-armv8.s
index bc3f146..fc9576c 100644
--- a/lib/accelerated/aarch64/elf/sha256-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha256-armv8.s
@@ -49,10 +49,10 @@
 .text
 
 
-.globl sha256_block_data_order
-.type sha256_block_data_order,%function
+.globl _gnutls_sha256_block_data_order
+.type _gnutls_sha256_block_data_order,%function
 .align 6
-sha256_block_data_order:
+_gnutls_sha256_block_data_order:
 
 
 
@@ -1029,7 +1029,7 @@ sha256_block_data_order:
  ldp x29,x30,[sp],#128
 .inst 0xd50323bf
  ret
-.size sha256_block_data_order,.-sha256_block_data_order
+.size _gnutls_sha256_block_data_order,.-_gnutls_sha256_block_data_order
 
 .align 6
 .type .LK256,%object
diff --git a/lib/accelerated/aarch64/elf/sha512-armv8.s b/lib/accelerated/aarch64/elf/sha512-armv8.s
index b036c2a..cdebd2a 100644
--- a/lib/accelerated/aarch64/elf/sha512-armv8.s
+++ b/lib/accelerated/aarch64/elf/sha512-armv8.s
@@ -49,10 +49,10 @@
 .text
 
 
-.globl sha512_block_data_order
-.type sha512_block_data_order,%function
+.globl _gnutls_sha512_block_data_order
+.type _gnutls_sha512_block_data_order,%function
 .align 6
-sha512_block_data_order:
+_gnutls_sha512_block_data_order:
 
 
 
@@ -1027,7 +1027,7 @@ sha512_block_data_order:
  ldp x29,x30,[sp],#128
 .inst 0xd50323bf
  ret
-.size sha512_block_data_order,.-sha512_block_data_order
+.size _gnutls_sha512_block_data_order,.-_gnutls_sha512_block_data_order
 
 .align 6
 .type .LK512,%object
diff --git a/lib/accelerated/aarch64/sha-aarch64.c b/lib/accelerated/aarch64/sha-aarch64.c
index e487129..3956729 100644
--- a/lib/accelerated/aarch64/sha-aarch64.c
+++ b/lib/accelerated/aarch64/sha-aarch64.c
@@ -31,9 +31,9 @@
 #include <sha-aarch64.h>
 #include <aarch64-common.h>
 
-void sha1_block_data_order(void *c, const void *p, size_t len);
-void sha256_block_data_order(void *c, const void *p, size_t len);
-void sha512_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha1_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha256_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha512_block_data_order(void *c, const void *p, size_t len);
 
 typedef void (*update_func) (void *, size_t, const uint8_t *);
 typedef void (*digest_func) (void *, size_t, uint8_t *);
@@ -108,7 +108,7 @@ void aarch64_sha1_update(struct sha1_ctx *ctx, size_t length,
 
 		t2 = length / SHA1_DATA_SIZE;
 
-		sha1_block_data_order(&octx, data, t2);
+		_gnutls_sha1_block_data_order(&octx, data, t2);
 
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -161,7 +161,7 @@ void aarch64_sha256_update(struct sha256_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA1_DATA_SIZE;
-		sha256_block_data_order(&octx, data, t2);
+		_gnutls_sha256_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -212,7 +212,7 @@ void aarch64_sha512_update(struct sha512_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA512_DATA_SIZE;
-		sha512_block_data_order(&octx, data, t2);
+		_gnutls_sha512_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			MD_INCR(ctx);
diff --git a/lib/accelerated/x86/aes-cbc-x86-aesni.c b/lib/accelerated/x86/aes-cbc-x86-aesni.c
index e4364d5..ce9078f 100644
--- a/lib/accelerated/x86/aes-cbc-x86-aesni.c
+++ b/lib/accelerated/x86/aes-cbc-x86-aesni.c
@@ -70,11 +70,11 @@ aes_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 
 	if (ctx->enc)
 		ret =
-		    aesni_set_encrypt_key(userkey, keysize * 8,
+		    _gnutls_aesni_set_encrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 	else
 		ret =
-		    aesni_set_decrypt_key(userkey, keysize * 8,
+		    _gnutls_aesni_set_decrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 
 	if (ret != 0)
@@ -103,7 +103,7 @@ aes_encrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 1);
 	return 0;
 }
@@ -117,7 +117,7 @@ aes_decrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_aesni_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 0);
 
 	return 0;
diff --git a/lib/accelerated/x86/aes-cbc-x86-ssse3.c b/lib/accelerated/x86/aes-cbc-x86-ssse3.c
index fe09f50..e5f4e95 100644
--- a/lib/accelerated/x86/aes-cbc-x86-ssse3.c
+++ b/lib/accelerated/x86/aes-cbc-x86-ssse3.c
@@ -70,11 +70,11 @@ aes_ssse3_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 
 	if (ctx->enc)
 		ret =
-		    vpaes_set_encrypt_key(userkey, keysize * 8,
+		    _gnutls_vpaes_set_encrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 	else
 		ret =
-		    vpaes_set_decrypt_key(userkey, keysize * 8,
+		    _gnutls_vpaes_set_decrypt_key(userkey, keysize * 8,
 					  ALIGN16(&ctx->expanded_key));
 
 	if (ret != 0)
@@ -92,7 +92,7 @@ aes_ssse3_encrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	vpaes_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_vpaes_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 1);
 	return 0;
 }
@@ -106,7 +106,7 @@ aes_ssse3_decrypt(void *_ctx, const void *src, size_t src_size,
 	if (unlikely(src_size % 16 != 0))
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
-	vpaes_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
+	_gnutls_vpaes_cbc_encrypt(src, dst, src_size, ALIGN16(&ctx->expanded_key),
 			  ctx->iv, 0);
 
 	return 0;
diff --git a/lib/accelerated/x86/aes-ccm-x86-aesni.c b/lib/accelerated/x86/aes-ccm-x86-aesni.c
index 95607b9..c159c0d 100644
--- a/lib/accelerated/x86/aes-ccm-x86-aesni.c
+++ b/lib/accelerated/x86/aes-ccm-x86-aesni.c
@@ -49,7 +49,7 @@ static void x86_aes_encrypt(const void *_ctx,
 			    const uint8_t * src)
 {
 	AES_KEY *ctx = (void*)_ctx;
-	aesni_ecb_encrypt(src, dst, length, ctx, 1);
+	_gnutls_aesni_ecb_encrypt(src, dst, length, ctx, 1);
 }
 
 static int
@@ -76,7 +76,7 @@ static int
 aes_ccm_cipher_setkey(void *_ctx, const void *key, size_t length)
 {
 	struct ccm_x86_aes_ctx *ctx = _ctx;
-	aesni_set_encrypt_key(key, length*8, &ctx->key);
+	_gnutls_aesni_set_encrypt_key(key, length*8, &ctx->key);
 
 	return 0;
 }
diff --git a/lib/accelerated/x86/aes-gcm-padlock.c b/lib/accelerated/x86/aes-gcm-padlock.c
index d651d0a..d1458c2 100644
--- a/lib/accelerated/x86/aes-gcm-padlock.c
+++ b/lib/accelerated/x86/aes-gcm-padlock.c
@@ -55,7 +55,7 @@ static void padlock_aes_encrypt(const void *_ctx,
 	pce = ALIGN16(&ctx->expanded_key);
 
 	if (length > 0)
-		padlock_ecb_encrypt(dst, src, pce, length);
+		_gnutls_padlock_ecb_encrypt(dst, src, pce, length);
 }
 
 static void padlock_aes128_set_encrypt_key(struct padlock_ctx *_ctx,
diff --git a/lib/accelerated/x86/aes-gcm-x86-aesni.c b/lib/accelerated/x86/aes-gcm-x86-aesni.c
index e5110ef..5858418 100644
--- a/lib/accelerated/x86/aes-gcm-x86-aesni.c
+++ b/lib/accelerated/x86/aes-gcm-x86-aesni.c
@@ -49,7 +49,7 @@ static void x86_aes_encrypt(const void *_ctx,
 {
 	AES_KEY *ctx = (void*)_ctx;
 
-	aesni_ecb_encrypt(src, dst, length, ctx, 1);
+	_gnutls_aesni_ecb_encrypt(src, dst, length, ctx, 1);
 }
 
 static void x86_aes128_set_encrypt_key(void *_ctx,
@@ -57,7 +57,7 @@ static void x86_aes128_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	aesni_set_encrypt_key(key, 16*8, ctx);
+	_gnutls_aesni_set_encrypt_key(key, 16*8, ctx);
 }
 
 static void x86_aes256_set_encrypt_key(void *_ctx,
@@ -65,7 +65,7 @@ static void x86_aes256_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	aesni_set_encrypt_key(key, 32*8, ctx);
+	_gnutls_aesni_set_encrypt_key(key, 32*8, ctx);
 }
 
 static int
diff --git a/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c b/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
index 747a894..18088bf 100644
--- a/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
+++ b/lib/accelerated/x86/aes-gcm-x86-pclmul-avx.c
@@ -63,9 +63,9 @@ struct aes_gcm_ctx {
 	unsigned auth_finished;
 };
 
-void gcm_init_avx(u128 Htable[16], const uint64_t Xi[2]);
-void gcm_ghash_avx(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in, size_t len);
-void gcm_gmult_avx(uint64_t Xi[2], const u128 Htable[16]);
+void _gnutls_gcm_init_avx(u128 Htable[16], const uint64_t Xi[2]);
+void _gnutls_gcm_ghash_avx(uint64_t Xi[2], const u128 Htable[16], const uint8_t *in, size_t len);
+void _gnutls_gcm_gmult_avx(uint64_t Xi[2], const u128 Htable[16]);
 
 static void aes_gcm_deinit(void *_ctx)
 {
@@ -102,18 +102,18 @@ aes_gcm_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 	CHECK_AES_KEYSIZE(keysize);
 
 	ret =
-	    aesni_set_encrypt_key(userkey, keysize * 8,
+	    _gnutls_aesni_set_encrypt_key(userkey, keysize * 8,
 				  ALIGN16(&ctx->expanded_key));
 	if (ret != 0)
 		return gnutls_assert_val(GNUTLS_E_ENCRYPTION_FAILED);
 
-	aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
+	_gnutls_aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 
 	ctx->gcm.H.u[0] = bswap_64(ctx->gcm.H.u[0]);
 	ctx->gcm.H.u[1] = bswap_64(ctx->gcm.H.u[1]);
 
-	gcm_init_avx(ctx->gcm.Htable, ctx->gcm.H.u);
+	_gnutls_gcm_init_avx(ctx->gcm.Htable, ctx->gcm.H.u);
 
 	return 0;
 }
@@ -134,7 +134,7 @@ static int aes_gcm_setiv(void *_ctx, const void *iv, size_t iv_size)
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 2] = 0;
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 1;
 
-	aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
+	_gnutls_aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 2;
 	ctx->finished = 0;
@@ -149,12 +149,12 @@ gcm_ghash(struct aes_gcm_ctx *ctx, const uint8_t * src, size_t src_size)
 	size_t aligned_size = src_size - rest;
 
 	if (aligned_size > 0)
-		gcm_ghash_avx(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
+		_gnutls_gcm_ghash_avx(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
 				aligned_size);
 
 	if (rest > 0) {
 		memxor(ctx->gcm.Xi.c, src + aligned_size, rest);
-		gcm_gmult_avx(ctx->gcm.Xi.u, ctx->gcm.Htable);
+		_gnutls_gcm_gmult_avx(ctx->gcm.Xi.u, ctx->gcm.Htable);
 	}
 }
 
@@ -166,7 +166,7 @@ ctr_encrypt_last(struct aes_gcm_ctx *ctx, const uint8_t * src,
 	uint8_t out[GCM_BLOCK_SIZE];
 
 	memcpy(tmp, &src[pos], length);
-	aesni_ctr32_encrypt_blocks(tmp, out, 1,
+	_gnutls_aesni_ctr32_encrypt_blocks(tmp, out, 1,
 				   ALIGN16(&ctx->expanded_key),
 				   ctx->gcm.Yi.c);
 
@@ -188,7 +188,7 @@ aes_gcm_encrypt(void *_ctx, const void *src, size_t src_size,
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_gnutls_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -226,7 +226,7 @@ aes_gcm_decrypt(void *_ctx, const void *src, size_t src_size,
 	ctx->gcm.len.u[1] += src_size;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_gnutls_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -273,7 +273,7 @@ static void aes_gcm_tag(void *_ctx, void *tag, size_t tagsize)
 	_gnutls_write_uint64(alen, buffer);
 	_gnutls_write_uint64(clen, &buffer[8]);
 
-	gcm_ghash_avx(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
+	_gnutls_gcm_ghash_avx(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
 			GCM_BLOCK_SIZE);
 
 	ctx->gcm.Xi.u[0] ^= ctx->gcm.EK0.u[0];
@@ -302,7 +302,7 @@ aesni_gcm_aead_encrypt(void *_ctx,
 	aes_gcm_auth(ctx, auth, auth_size);
 
 	if (plain_size >= 96) {
-		s = aesni_gcm_encrypt(plain, encr, plain_size, ALIGN16(&ctx->expanded_key),
+		s = _gnutls_aesni_gcm_encrypt(plain, encr, plain_size, ALIGN16(&ctx->expanded_key),
 			ctx->gcm.Yi.c, ctx->gcm.Xi.u);
 		ctx->gcm.len.u[1] += s;
 	}
@@ -336,7 +336,7 @@ aesni_gcm_aead_decrypt(void *_ctx,
 	encr_size -= tag_size;
 
 	if (encr_size >= 96) {
-		s = aesni_gcm_decrypt(encr, plain, encr_size, ALIGN16(&ctx->expanded_key),
+		s = _gnutls_aesni_gcm_decrypt(encr, plain, encr_size, ALIGN16(&ctx->expanded_key),
 			ctx->gcm.Yi.c, ctx->gcm.Xi.u);
 		ctx->gcm.len.u[1] += s;
 	}
diff --git a/lib/accelerated/x86/aes-gcm-x86-pclmul.c b/lib/accelerated/x86/aes-gcm-x86-pclmul.c
index 2225b93..cfa0793 100644
--- a/lib/accelerated/x86/aes-gcm-x86-pclmul.c
+++ b/lib/accelerated/x86/aes-gcm-x86-pclmul.c
@@ -62,10 +62,10 @@ struct aes_gcm_ctx {
 	unsigned auth_finished;
 };
 
-void gcm_init_clmul(u128 Htable[16], const uint64_t Xi[2]);
-void gcm_ghash_clmul(uint64_t Xi[2], const u128 Htable[16],
+void _gnutls_gcm_init_clmul(u128 Htable[16], const uint64_t Xi[2]);
+void _gnutls_gcm_ghash_clmul(uint64_t Xi[2], const u128 Htable[16],
 		     const uint8_t * inp, size_t len);
-void gcm_gmult_clmul(uint64_t Xi[2], const u128 Htable[16]);
+void _gnutls_gcm_gmult_clmul(uint64_t Xi[2], const u128 Htable[16]);
 
 static void aes_gcm_deinit(void *_ctx)
 {
@@ -102,18 +102,18 @@ aes_gcm_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 	CHECK_AES_KEYSIZE(keysize);
 
 	ret =
-	    aesni_set_encrypt_key(userkey, keysize * 8,
+	    _gnutls_aesni_set_encrypt_key(userkey, keysize * 8,
 				  ALIGN16(&ctx->expanded_key));
 	if (ret != 0)
 		return gnutls_assert_val(GNUTLS_E_ENCRYPTION_FAILED);
 
-	aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
+	_gnutls_aesni_ecb_encrypt(ctx->gcm.H.c, ctx->gcm.H.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 
 	ctx->gcm.H.u[0] = bswap_64(ctx->gcm.H.u[0]);
 	ctx->gcm.H.u[1] = bswap_64(ctx->gcm.H.u[1]);
 
-	gcm_init_clmul(ctx->gcm.Htable, ctx->gcm.H.u);
+	_gnutls_gcm_init_clmul(ctx->gcm.Htable, ctx->gcm.H.u);
 
 	return 0;
 }
@@ -134,7 +134,7 @@ static int aes_gcm_setiv(void *_ctx, const void *iv, size_t iv_size)
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 2] = 0;
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 1;
 
-	aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
+	_gnutls_aesni_ecb_encrypt(ctx->gcm.Yi.c, ctx->gcm.EK0.c,
 			  GCM_BLOCK_SIZE, ALIGN16(&ctx->expanded_key), 1);
 	ctx->gcm.Yi.c[GCM_BLOCK_SIZE - 1] = 2;
 	ctx->finished = 0;
@@ -149,12 +149,12 @@ gcm_ghash(struct aes_gcm_ctx *ctx, const uint8_t * src, size_t src_size)
 	size_t aligned_size = src_size - rest;
 
 	if (aligned_size > 0)
-		gcm_ghash_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
+		_gnutls_gcm_ghash_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable, src,
 				aligned_size);
 
 	if (rest > 0) {
 		memxor(ctx->gcm.Xi.c, src + aligned_size, rest);
-		gcm_gmult_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable);
+		_gnutls_gcm_gmult_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable);
 	}
 }
 
@@ -166,7 +166,7 @@ ctr_encrypt_last(struct aes_gcm_ctx *ctx, const uint8_t * src,
 	uint8_t out[GCM_BLOCK_SIZE];
 
 	memcpy(tmp, &src[pos], length);
-	aesni_ctr32_encrypt_blocks(tmp, out, 1,
+	_gnutls_aesni_ctr32_encrypt_blocks(tmp, out, 1,
 				   ALIGN16(&ctx->expanded_key),
 				   ctx->gcm.Yi.c);
 
@@ -188,7 +188,7 @@ aes_gcm_encrypt(void *_ctx, const void *src, size_t src_size,
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_gnutls_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -226,7 +226,7 @@ aes_gcm_decrypt(void *_ctx, const void *src, size_t src_size,
 	ctx->gcm.len.u[1] += src_size;
 
 	if (blocks > 0) {
-		aesni_ctr32_encrypt_blocks(src, dst,
+		_gnutls_aesni_ctr32_encrypt_blocks(src, dst,
 					   blocks,
 					   ALIGN16(&ctx->expanded_key),
 					   ctx->gcm.Yi.c);
@@ -273,7 +273,7 @@ static void aes_gcm_tag(void *_ctx, void *tag, size_t tagsize)
 	_gnutls_write_uint64(alen, buffer);
 	_gnutls_write_uint64(clen, &buffer[8]);
 
-	gcm_ghash_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
+	_gnutls_gcm_ghash_clmul(ctx->gcm.Xi.u, ctx->gcm.Htable, buffer,
 			GCM_BLOCK_SIZE);
 
 	ctx->gcm.Xi.u[0] ^= ctx->gcm.EK0.u[0];
diff --git a/lib/accelerated/x86/aes-gcm-x86-ssse3.c b/lib/accelerated/x86/aes-gcm-x86-ssse3.c
index 5580cc2..5c229d9 100644
--- a/lib/accelerated/x86/aes-gcm-x86-ssse3.c
+++ b/lib/accelerated/x86/aes-gcm-x86-ssse3.c
@@ -55,7 +55,7 @@ static void x86_aes_encrypt(const void *_ctx,
 	assert(blocks*16 == length);
 
 	for (i=0;i<blocks;i++) {
-		vpaes_encrypt(src, dst, ctx);
+		_gnutls_vpaes_encrypt(src, dst, ctx);
 		dst += 16;
 		src += 16;
 	}
@@ -66,7 +66,7 @@ static void x86_aes_128_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	vpaes_set_encrypt_key(key, 16*8, ctx);
+	_gnutls_vpaes_set_encrypt_key(key, 16*8, ctx);
 }
 
 static void x86_aes_256_set_encrypt_key(void *_ctx,
@@ -74,7 +74,7 @@ static void x86_aes_256_set_encrypt_key(void *_ctx,
 {
 	AES_KEY *ctx = _ctx;
 
-	vpaes_set_encrypt_key(key, 32*8, ctx);
+	_gnutls_vpaes_set_encrypt_key(key, 32*8, ctx);
 }
 
 static int
diff --git a/lib/accelerated/x86/aes-padlock.c b/lib/accelerated/x86/aes-padlock.c
index f10b5c5..52055d7 100644
--- a/lib/accelerated/x86/aes-padlock.c
+++ b/lib/accelerated/x86/aes-padlock.c
@@ -93,7 +93,7 @@ padlock_aes_cipher_setkey(void *_ctx, const void *userkey, size_t keysize)
 		return gnutls_assert_val(GNUTLS_E_INVALID_REQUEST);
 	}
 
-	padlock_reload_key();
+	_gnutls_padlock_reload_key();
 
 	return 0;
 }
@@ -123,7 +123,7 @@ padlock_aes_cbc_encrypt(void *_ctx, const void *src, size_t src_size,
 	pce = ALIGN16(&ctx->expanded_key);
 
 	if (src_size > 0)
-		padlock_cbc_encrypt(dst, src, pce, src_size);
+		_gnutls_padlock_cbc_encrypt(dst, src, pce, src_size);
 
 	return 0;
 }
@@ -139,7 +139,7 @@ padlock_aes_cbc_decrypt(void *_ctx, const void *src, size_t src_size,
 	pcd = ALIGN16(&ctx->expanded_key);
 
 	if (src_size > 0)
-		padlock_cbc_encrypt(dst, src, pcd, src_size);
+		_gnutls_padlock_cbc_encrypt(dst, src, pcd, src_size);
 
 	return 0;
 }
diff --git a/lib/accelerated/x86/aes-padlock.h b/lib/accelerated/x86/aes-padlock.h
index cd9dc23..604013e 100644
--- a/lib/accelerated/x86/aes-padlock.h
+++ b/lib/accelerated/x86/aes-padlock.h
@@ -37,10 +37,10 @@ int padlock_aes_cipher_setkey(void *_ctx, const void *userkey,
 			      size_t keysize);
 
 /* asm */
-unsigned int padlock_capability(void);
-void padlock_reload_key(void);
-int padlock_ecb_encrypt(void *out, const void *inp,
+unsigned int _gnutls_padlock_capability(void);
+void _gnutls_padlock_reload_key(void);
+int _gnutls_padlock_ecb_encrypt(void *out, const void *inp,
 			struct padlock_cipher_data *ctx, size_t len);
-int padlock_cbc_encrypt(void *out, const void *inp,
+int _gnutls_padlock_cbc_encrypt(void *out, const void *inp,
 			struct padlock_cipher_data *ctx, size_t len);
 #endif /* GNUTLS_LIB_ACCELERATED_X86_AES_PADLOCK_H */
diff --git a/lib/accelerated/x86/aes-x86.h b/lib/accelerated/x86/aes-x86.h
index 92f54a6..8e36e08 100644
--- a/lib/accelerated/x86/aes-x86.h
+++ b/lib/accelerated/x86/aes-x86.h
@@ -22,35 +22,35 @@ typedef struct {
 	if (s != 16 && s != 24 && s != 32) \
 		return GNUTLS_E_INVALID_REQUEST
 
-void aesni_ecb_encrypt(const unsigned char *in, unsigned char *out,
+void _gnutls_aesni_ecb_encrypt(const unsigned char *in, unsigned char *out,
 		       size_t len, const AES_KEY * key, int enc);
 
-void aesni_cbc_encrypt(const unsigned char *in, unsigned char *out,
+void _gnutls_aesni_cbc_encrypt(const unsigned char *in, unsigned char *out,
 		       size_t len, const AES_KEY * key,
 		       unsigned char *ivec, const int enc);
-int aesni_set_decrypt_key(const unsigned char *userKey, const int bits,
+int _gnutls_aesni_set_decrypt_key(const unsigned char *userKey, const int bits,
 			  AES_KEY * key);
-int aesni_set_encrypt_key(const unsigned char *userKey, const int bits,
+int _gnutls_aesni_set_encrypt_key(const unsigned char *userKey, const int bits,
 			  AES_KEY * key);
 
-void aesni_ctr32_encrypt_blocks(const unsigned char *in,
+void _gnutls_aesni_ctr32_encrypt_blocks(const unsigned char *in,
 				unsigned char *out,
 				size_t blocks,
 				const void *key,
 				const unsigned char *ivec);
 
-size_t aesni_gcm_encrypt(const void *inp, void *out, size_t len,
+size_t _gnutls_aesni_gcm_encrypt(const void *inp, void *out, size_t len,
 	const AES_KEY *key, const unsigned char iv[16], uint64_t* Xi);
 
-size_t aesni_gcm_decrypt(const void *inp, void *out, size_t len,
+size_t _gnutls_aesni_gcm_decrypt(const void *inp, void *out, size_t len,
 	const AES_KEY *key, const unsigned char iv[16], uint64_t* Xi);
 
-int vpaes_set_encrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);  
-int vpaes_set_decrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);
-void vpaes_cbc_encrypt(const unsigned char *in, unsigned char *out,
+int _gnutls_vpaes_set_encrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);  
+int _gnutls_vpaes_set_decrypt_key(const unsigned char *userKey, int bits, AES_KEY *key);
+void _gnutls_vpaes_cbc_encrypt(const unsigned char *in, unsigned char *out,
                        size_t length, const AES_KEY *key, unsigned char *ivec, int enc);
-void vpaes_encrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
-void vpaes_decrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
+void _gnutls_vpaes_encrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
+void _gnutls_vpaes_decrypt(const unsigned char *in, unsigned char *out, const AES_KEY *key);
 
 extern const gnutls_crypto_cipher_st _gnutls_aes_gcm_pclmul;
 extern const gnutls_crypto_cipher_st _gnutls_aes_gcm_pclmul_avx;
diff --git a/lib/accelerated/x86/coff/aes-ssse3-x86.s b/lib/accelerated/x86/coff/aes-ssse3-x86.s
index c58ea23..4f9a86f 100644
--- a/lib/accelerated/x86/coff/aes-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/aes-ssse3-x86.s
@@ -462,10 +462,10 @@ __vpaes_schedule_mangle:
 	andl	$48,%ecx
 	movdqu	%xmm3,(%edx)
 	ret
-.globl	_vpaes_set_encrypt_key
-.def	_vpaes_set_encrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_vpaes_set_encrypt_key
+.def	__gnutls_vpaes_set_encrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_vpaes_set_encrypt_key:
+__gnutls_vpaes_set_encrypt_key:
 .L_vpaes_set_encrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -494,10 +494,10 @@ _vpaes_set_encrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_set_decrypt_key
-.def	_vpaes_set_decrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_vpaes_set_decrypt_key
+.def	__gnutls_vpaes_set_decrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_vpaes_set_decrypt_key:
+__gnutls_vpaes_set_decrypt_key:
 .L_vpaes_set_decrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -531,10 +531,10 @@ _vpaes_set_decrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_encrypt
-.def	_vpaes_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_vpaes_encrypt
+.def	__gnutls_vpaes_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_vpaes_encrypt:
+__gnutls_vpaes_encrypt:
 .L_vpaes_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -559,10 +559,10 @@ _vpaes_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_decrypt
-.def	_vpaes_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_vpaes_decrypt
+.def	__gnutls_vpaes_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_vpaes_decrypt:
+__gnutls_vpaes_decrypt:
 .L_vpaes_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -587,10 +587,10 @@ _vpaes_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_cbc_encrypt
-.def	_vpaes_cbc_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_vpaes_cbc_encrypt
+.def	__gnutls_vpaes_cbc_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_vpaes_cbc_encrypt:
+__gnutls_vpaes_cbc_encrypt:
 .L_vpaes_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/coff/aes-ssse3-x86_64.s b/lib/accelerated/x86/coff/aes-ssse3-x86_64.s
index 150c992..99d41d4 100644
--- a/lib/accelerated/x86/coff/aes-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/aes-ssse3-x86_64.s
@@ -630,10 +630,10 @@ _vpaes_schedule_mangle:
 
 
 
-.globl	vpaes_set_encrypt_key
-.def	vpaes_set_encrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_vpaes_set_encrypt_key
+.def	_gnutls_vpaes_set_encrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-vpaes_set_encrypt_key:
+_gnutls_vpaes_set_encrypt_key:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -682,10 +682,10 @@ vpaes_set_encrypt_key:
 
 .LSEH_end_vpaes_set_encrypt_key:
 
-.globl	vpaes_set_decrypt_key
-.def	vpaes_set_decrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_vpaes_set_decrypt_key
+.def	_gnutls_vpaes_set_decrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-vpaes_set_decrypt_key:
+_gnutls_vpaes_set_decrypt_key:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -739,10 +739,10 @@ vpaes_set_decrypt_key:
 
 .LSEH_end_vpaes_set_decrypt_key:
 
-.globl	vpaes_encrypt
-.def	vpaes_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_vpaes_encrypt
+.def	_gnutls_vpaes_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-vpaes_encrypt:
+_gnutls_vpaes_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -786,10 +786,10 @@ vpaes_encrypt:
 
 .LSEH_end_vpaes_encrypt:
 
-.globl	vpaes_decrypt
-.def	vpaes_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_vpaes_decrypt
+.def	_gnutls_vpaes_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-vpaes_decrypt:
+_gnutls_vpaes_decrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -832,10 +832,10 @@ vpaes_decrypt:
 	.byte	0xf3,0xc3
 
 .LSEH_end_vpaes_decrypt:
-.globl	vpaes_cbc_encrypt
-.def	vpaes_cbc_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_vpaes_cbc_encrypt
+.def	_gnutls_vpaes_cbc_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-vpaes_cbc_encrypt:
+_gnutls_vpaes_cbc_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/aesni-gcm-x86_64.s b/lib/accelerated/x86/coff/aesni-gcm-x86_64.s
index 7988004..c905aca 100644
--- a/lib/accelerated/x86/coff/aesni-gcm-x86_64.s
+++ b/lib/accelerated/x86/coff/aesni-gcm-x86_64.s
@@ -350,10 +350,10 @@ _aesni_ctr32_ghash_6x:
 
 	.byte	0xf3,0xc3
 
-.globl	aesni_gcm_decrypt
-.def	aesni_gcm_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_gcm_decrypt
+.def	_gnutls_aesni_gcm_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	5
-aesni_gcm_decrypt:
+_gnutls_aesni_gcm_decrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -578,10 +578,10 @@ _aesni_ctr32_6x:
 	jmp	.Loop_ctr32
 
 
-.globl	aesni_gcm_encrypt
-.def	aesni_gcm_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_gcm_encrypt
+.def	_gnutls_aesni_gcm_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	5
-aesni_gcm_encrypt:
+_gnutls_aesni_gcm_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/aesni-x86.s b/lib/accelerated/x86/coff/aesni-x86.s
index c6aa1a1..d8c3c0b 100644
--- a/lib/accelerated/x86/coff/aesni-x86.s
+++ b/lib/accelerated/x86/coff/aesni-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_aesni_encrypt
-.def	_aesni_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_encrypt
+.def	__gnutls_aesni_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_encrypt:
+__gnutls_aesni_encrypt:
 .L_aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -64,10 +64,10 @@ _aesni_encrypt:
 	movups	%xmm2,(%eax)
 	pxor	%xmm2,%xmm2
 	ret
-.globl	_aesni_decrypt
-.def	_aesni_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_decrypt
+.def	__gnutls_aesni_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_decrypt:
+__gnutls_aesni_decrypt:
 .L_aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -384,10 +384,10 @@ __aesni_decrypt6:
 .byte	102,15,56,223,240
 .byte	102,15,56,223,248
 	ret
-.globl	_aesni_ecb_encrypt
-.def	_aesni_ecb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ecb_encrypt
+.def	__gnutls_aesni_ecb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ecb_encrypt:
+__gnutls_aesni_ecb_encrypt:
 .L_aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -618,10 +618,10 @@ _aesni_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_encrypt_blocks
-.def	_aesni_ccm64_encrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ccm64_encrypt_blocks
+.def	__gnutls_aesni_ccm64_encrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ccm64_encrypt_blocks:
+__gnutls_aesni_ccm64_encrypt_blocks:
 .L_aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -705,10 +705,10 @@ _aesni_ccm64_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_decrypt_blocks
-.def	_aesni_ccm64_decrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ccm64_decrypt_blocks
+.def	__gnutls_aesni_ccm64_decrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ccm64_decrypt_blocks:
+__gnutls_aesni_ccm64_decrypt_blocks:
 .L_aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -827,10 +827,10 @@ _aesni_ccm64_decrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ctr32_encrypt_blocks
-.def	_aesni_ctr32_encrypt_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ctr32_encrypt_blocks
+.def	__gnutls_aesni_ctr32_encrypt_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ctr32_encrypt_blocks:
+__gnutls_aesni_ctr32_encrypt_blocks:
 .L_aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1064,10 +1064,10 @@ _aesni_ctr32_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_encrypt
-.def	_aesni_xts_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_xts_encrypt
+.def	__gnutls_aesni_xts_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_xts_encrypt:
+__gnutls_aesni_xts_encrypt:
 .L_aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1423,10 +1423,10 @@ _aesni_xts_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_decrypt
-.def	_aesni_xts_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_xts_decrypt
+.def	__gnutls_aesni_xts_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_xts_decrypt:
+__gnutls_aesni_xts_decrypt:
 .L_aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1812,10 +1812,10 @@ _aesni_xts_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ocb_encrypt
-.def	_aesni_ocb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ocb_encrypt
+.def	__gnutls_aesni_ocb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ocb_encrypt:
+__gnutls_aesni_ocb_encrypt:
 .L_aesni_ocb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2206,10 +2206,10 @@ _aesni_ocb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ocb_decrypt
-.def	_aesni_ocb_decrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_ocb_decrypt
+.def	__gnutls_aesni_ocb_decrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_ocb_decrypt:
+__gnutls_aesni_ocb_decrypt:
 .L_aesni_ocb_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2600,10 +2600,10 @@ _aesni_ocb_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_cbc_encrypt
-.def	_aesni_cbc_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_cbc_encrypt
+.def	__gnutls_aesni_cbc_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_cbc_encrypt:
+__gnutls_aesni_cbc_encrypt:
 .L_aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -3192,20 +3192,20 @@ __aesni_set_encrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_set_encrypt_key
-.def	_aesni_set_encrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_set_encrypt_key
+.def	__gnutls_aesni_set_encrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_set_encrypt_key:
+__gnutls_aesni_set_encrypt_key:
 .L_aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
 	call	__aesni_set_encrypt_key
 	ret
-.globl	_aesni_set_decrypt_key
-.def	_aesni_set_decrypt_key;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_aesni_set_decrypt_key
+.def	__gnutls_aesni_set_decrypt_key;	.scl	2;	.type	32;	.endef
 .align	16
-_aesni_set_decrypt_key:
+__gnutls_aesni_set_decrypt_key:
 .L_aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
diff --git a/lib/accelerated/x86/coff/aesni-x86_64.s b/lib/accelerated/x86/coff/aesni-x86_64.s
index 4e8de06..131cdc9 100644
--- a/lib/accelerated/x86/coff/aesni-x86_64.s
+++ b/lib/accelerated/x86/coff/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	aesni_encrypt
-.def	aesni_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_encrypt
+.def	_gnutls_aesni_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_encrypt:
+_gnutls_aesni_encrypt:
 
 	movups	(%rcx),%xmm2
 	movl	240(%r8),%eax
@@ -65,10 +65,10 @@ aesni_encrypt:
 
 
 
-.globl	aesni_decrypt
-.def	aesni_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_decrypt
+.def	_gnutls_aesni_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_decrypt:
+_gnutls_aesni_decrypt:
 
 	movups	(%rcx),%xmm2
 	movl	240(%r8),%eax
@@ -552,10 +552,10 @@ _aesni_decrypt8:
 	.byte	0xf3,0xc3
 
 
-.globl	aesni_ecb_encrypt
-.def	aesni_ecb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ecb_encrypt
+.def	_gnutls_aesni_ecb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ecb_encrypt:
+_gnutls_aesni_ecb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -924,10 +924,10 @@ aesni_ecb_encrypt:
 	.byte	0xf3,0xc3
 
 .LSEH_end_aesni_ecb_encrypt:
-.globl	aesni_ccm64_encrypt_blocks
-.def	aesni_ccm64_encrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ccm64_encrypt_blocks
+.def	_gnutls_aesni_ccm64_encrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ccm64_encrypt_blocks:
+_gnutls_aesni_ccm64_encrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1016,10 +1016,10 @@ aesni_ccm64_encrypt_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ccm64_encrypt_blocks:
-.globl	aesni_ccm64_decrypt_blocks
-.def	aesni_ccm64_decrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ccm64_decrypt_blocks
+.def	_gnutls_aesni_ccm64_decrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ccm64_decrypt_blocks:
+_gnutls_aesni_ccm64_decrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1142,10 +1142,10 @@ aesni_ccm64_decrypt_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_aesni_ccm64_decrypt_blocks:
-.globl	aesni_ctr32_encrypt_blocks
-.def	aesni_ctr32_encrypt_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ctr32_encrypt_blocks
+.def	_gnutls_aesni_ctr32_encrypt_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_ctr32_encrypt_blocks:
+_gnutls_aesni_ctr32_encrypt_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -1753,10 +1753,10 @@ aesni_ctr32_encrypt_blocks:
 	.byte	0xf3,0xc3
 
 .LSEH_end_aesni_ctr32_encrypt_blocks:
-.globl	aesni_xts_encrypt
-.def	aesni_xts_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_xts_encrypt
+.def	_gnutls_aesni_xts_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_xts_encrypt:
+_gnutls_aesni_xts_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -2257,10 +2257,10 @@ aesni_xts_encrypt:
 	.byte	0xf3,0xc3
 
 .LSEH_end_aesni_xts_encrypt:
-.globl	aesni_xts_decrypt
-.def	aesni_xts_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_xts_decrypt
+.def	_gnutls_aesni_xts_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_xts_decrypt:
+_gnutls_aesni_xts_decrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -2798,10 +2798,10 @@ aesni_xts_decrypt:
 	.byte	0xf3,0xc3
 
 .LSEH_end_aesni_xts_decrypt:
-.globl	aesni_ocb_encrypt
-.def	aesni_ocb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ocb_encrypt
+.def	_gnutls_aesni_ocb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	5
-aesni_ocb_encrypt:
+_gnutls_aesni_ocb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -3249,10 +3249,10 @@ __ocb_encrypt1:
 	.byte	0xf3,0xc3
 
 
-.globl	aesni_ocb_decrypt
-.def	aesni_ocb_decrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_ocb_decrypt
+.def	_gnutls_aesni_ocb_decrypt;	.scl 2;	.type 32;	.endef
 .p2align	5
-aesni_ocb_decrypt:
+_gnutls_aesni_ocb_decrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -3710,10 +3710,10 @@ __ocb_decrypt1:
 .byte	102,15,56,223,215
 	.byte	0xf3,0xc3
 
-.globl	aesni_cbc_encrypt
-.def	aesni_cbc_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_cbc_encrypt
+.def	_gnutls_aesni_cbc_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_cbc_encrypt:
+_gnutls_aesni_cbc_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -4341,10 +4341,10 @@ aesni_cbc_encrypt:
 	.byte	0xf3,0xc3
 
 .LSEH_end_aesni_cbc_encrypt:
-.globl	aesni_set_decrypt_key
-.def	aesni_set_decrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_set_decrypt_key
+.def	_gnutls_aesni_set_decrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_set_decrypt_key:
+_gnutls_aesni_set_decrypt_key:
 
 .byte	0x48,0x83,0xEC,0x08
 
@@ -4385,10 +4385,10 @@ aesni_set_decrypt_key:
 
 .LSEH_end_set_decrypt_key:
 
-.globl	aesni_set_encrypt_key
-.def	aesni_set_encrypt_key;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_aesni_set_encrypt_key
+.def	_gnutls_aesni_set_encrypt_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-aesni_set_encrypt_key:
+_gnutls_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
 
 .byte	0x48,0x83,0xEC,0x08
@@ -5046,11 +5046,11 @@ cbc_se_handler:
 .rva	.LSEH_end_aesni_cbc_encrypt
 .rva	.LSEH_info_cbc
 
-.rva	aesni_set_decrypt_key
+.rva	_gnutls_aesni_set_decrypt_key
 .rva	.LSEH_end_set_decrypt_key
 .rva	.LSEH_info_key
 
-.rva	aesni_set_encrypt_key
+.rva	_gnutls_aesni_set_encrypt_key
 .rva	.LSEH_end_set_encrypt_key
 .rva	.LSEH_info_key
 .section	.xdata
diff --git a/lib/accelerated/x86/coff/e_padlock-x86.s b/lib/accelerated/x86/coff/e_padlock-x86.s
index 41f87b1..208053b 100644
--- a/lib/accelerated/x86/coff/e_padlock-x86.s
+++ b/lib/accelerated/x86/coff/e_padlock-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"devel/perlasm/e_padlock-x86.s"
 .text
-.globl	_padlock_capability
-.def	_padlock_capability;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_capability
+.def	__gnutls_padlock_capability;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_capability:
+__gnutls_padlock_capability:
 .L_padlock_capability_begin:
 	pushl	%ebx
 	pushfl
@@ -89,10 +89,10 @@ _padlock_capability:
 .L000noluck:
 	popl	%ebx
 	ret
-.globl	_padlock_key_bswap
-.def	_padlock_key_bswap;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_key_bswap
+.def	__gnutls_padlock_key_bswap;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_key_bswap:
+__gnutls_padlock_key_bswap:
 .L_padlock_key_bswap_begin:
 	movl	4(%esp),%edx
 	movl	240(%edx),%ecx
@@ -104,10 +104,10 @@ _padlock_key_bswap:
 	subl	$1,%ecx
 	jnz	.L001bswap_loop
 	ret
-.globl	_padlock_verify_context
-.def	_padlock_verify_context;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_verify_context
+.def	__gnutls_padlock_verify_context;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_verify_context:
+__gnutls_padlock_verify_context:
 .L_padlock_verify_context_begin:
 	movl	4(%esp),%edx
 	leal	.Lpadlock_saved_context,%eax
@@ -128,18 +128,18 @@ __padlock_verify_ctx:
 .L003verified:
 	movl	%edx,(%eax)
 	ret
-.globl	_padlock_reload_key
-.def	_padlock_reload_key;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_reload_key
+.def	__gnutls_padlock_reload_key;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_reload_key:
+__gnutls_padlock_reload_key:
 .L_padlock_reload_key_begin:
 	pushfl
 	popfl
 	ret
-.globl	_padlock_aes_block
-.def	_padlock_aes_block;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_aes_block
+.def	__gnutls_padlock_aes_block;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_aes_block:
+__gnutls_padlock_aes_block:
 .L_padlock_aes_block_begin:
 	pushl	%edi
 	pushl	%esi
@@ -155,10 +155,10 @@ _padlock_aes_block:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_ecb_encrypt
-.def	_padlock_ecb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_ecb_encrypt
+.def	__gnutls_padlock_ecb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_ecb_encrypt:
+__gnutls_padlock_ecb_encrypt:
 .L_padlock_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -333,10 +333,10 @@ _padlock_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_cbc_encrypt
-.def	_padlock_cbc_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_cbc_encrypt
+.def	__gnutls_padlock_cbc_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_cbc_encrypt:
+__gnutls_padlock_cbc_encrypt:
 .L_padlock_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -515,10 +515,10 @@ _padlock_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_cfb_encrypt
-.def	_padlock_cfb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_cfb_encrypt
+.def	__gnutls_padlock_cfb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_cfb_encrypt:
+__gnutls_padlock_cfb_encrypt:
 .L_padlock_cfb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -636,10 +636,10 @@ _padlock_cfb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_ofb_encrypt
-.def	_padlock_ofb_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_ofb_encrypt
+.def	__gnutls_padlock_ofb_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_ofb_encrypt:
+__gnutls_padlock_ofb_encrypt:
 .L_padlock_ofb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -757,10 +757,10 @@ _padlock_ofb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_ctr32_encrypt
-.def	_padlock_ctr32_encrypt;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_ctr32_encrypt
+.def	__gnutls_padlock_ctr32_encrypt;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_ctr32_encrypt:
+__gnutls_padlock_ctr32_encrypt:
 .L_padlock_ctr32_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -863,10 +863,10 @@ _padlock_ctr32_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_xstore
-.def	_padlock_xstore;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_xstore
+.def	__gnutls_padlock_xstore;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_xstore:
+__gnutls_padlock_xstore:
 .L_padlock_xstore_begin:
 	pushl	%edi
 	movl	8(%esp),%edi
@@ -886,10 +886,10 @@ __win32_segv_handler:
 	movl	$0,%eax
 .L053ret:
 	ret
-.globl	_padlock_sha1_oneshot
-.def	_padlock_sha1_oneshot;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_sha1_oneshot
+.def	__gnutls_padlock_sha1_oneshot;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_sha1_oneshot:
+__gnutls_padlock_sha1_oneshot:
 .L_padlock_sha1_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -921,10 +921,10 @@ _padlock_sha1_oneshot:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha1_blocks
-.def	_padlock_sha1_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_sha1_blocks
+.def	__gnutls_padlock_sha1_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_sha1_blocks:
+__gnutls_padlock_sha1_blocks:
 .L_padlock_sha1_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -950,10 +950,10 @@ _padlock_sha1_blocks:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha256_oneshot
-.def	_padlock_sha256_oneshot;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_sha256_oneshot
+.def	__gnutls_padlock_sha256_oneshot;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_sha256_oneshot:
+__gnutls_padlock_sha256_oneshot:
 .L_padlock_sha256_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -985,10 +985,10 @@ _padlock_sha256_oneshot:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha256_blocks
-.def	_padlock_sha256_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_sha256_blocks
+.def	__gnutls_padlock_sha256_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_sha256_blocks:
+__gnutls_padlock_sha256_blocks:
 .L_padlock_sha256_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -1014,10 +1014,10 @@ _padlock_sha256_blocks:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha512_blocks
-.def	_padlock_sha512_blocks;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_padlock_sha512_blocks
+.def	__gnutls_padlock_sha512_blocks;	.scl	2;	.type	32;	.endef
 .align	16
-_padlock_sha512_blocks:
+__gnutls_padlock_sha512_blocks:
 .L_padlock_sha512_blocks_begin:
 	pushl	%edi
 	pushl	%esi
diff --git a/lib/accelerated/x86/coff/e_padlock-x86_64.s b/lib/accelerated/x86/coff/e_padlock-x86_64.s
index 7edee19..8631687 100644
--- a/lib/accelerated/x86/coff/e_padlock-x86_64.s
+++ b/lib/accelerated/x86/coff/e_padlock-x86_64.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text	
-.globl	padlock_capability
-.def	padlock_capability;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_capability
+.def	_gnutls_padlock_capability;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_capability:
+_gnutls_padlock_capability:
 	movq	%rbx,%r8
 	xorl	%eax,%eax
 	cpuid
@@ -68,10 +68,10 @@ padlock_capability:
 	.byte	0xf3,0xc3
 
 
-.globl	padlock_key_bswap
-.def	padlock_key_bswap;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_key_bswap
+.def	_gnutls_padlock_key_bswap;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_key_bswap:
+_gnutls_padlock_key_bswap:
 	movl	240(%rcx),%edx
 .Lbswap_loop:
 	movl	(%rcx),%eax
@@ -83,10 +83,10 @@ padlock_key_bswap:
 	.byte	0xf3,0xc3
 
 
-.globl	padlock_verify_context
-.def	padlock_verify_context;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_verify_context
+.def	_gnutls_padlock_verify_context;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_verify_context:
+_gnutls_padlock_verify_context:
 	movq	%rcx,%rdx
 	pushf
 	leaq	.Lpadlock_saved_context(%rip),%rax
@@ -110,19 +110,19 @@ _padlock_verify_ctx:
 	.byte	0xf3,0xc3
 
 
-.globl	padlock_reload_key
-.def	padlock_reload_key;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_reload_key
+.def	_gnutls_padlock_reload_key;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_reload_key:
+_gnutls_padlock_reload_key:
 	pushf
 	popf
 	.byte	0xf3,0xc3
 
 
-.globl	padlock_aes_block
-.def	padlock_aes_block;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_aes_block
+.def	_gnutls_padlock_aes_block;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_aes_block:
+_gnutls_padlock_aes_block:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -142,10 +142,10 @@ padlock_aes_block:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_aes_block:
 
-.globl	padlock_xstore
-.def	padlock_xstore;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_xstore
+.def	_gnutls_padlock_xstore;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_xstore:
+_gnutls_padlock_xstore:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -160,10 +160,10 @@ padlock_xstore:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_xstore:
 
-.globl	padlock_sha1_oneshot
-.def	padlock_sha1_oneshot;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_sha1_oneshot
+.def	_gnutls_padlock_sha1_oneshot;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_sha1_oneshot:
+_gnutls_padlock_sha1_oneshot:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -192,10 +192,10 @@ padlock_sha1_oneshot:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_sha1_oneshot:
 
-.globl	padlock_sha1_blocks
-.def	padlock_sha1_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_sha1_blocks
+.def	_gnutls_padlock_sha1_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_sha1_blocks:
+_gnutls_padlock_sha1_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -224,10 +224,10 @@ padlock_sha1_blocks:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_sha1_blocks:
 
-.globl	padlock_sha256_oneshot
-.def	padlock_sha256_oneshot;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_sha256_oneshot
+.def	_gnutls_padlock_sha256_oneshot;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_sha256_oneshot:
+_gnutls_padlock_sha256_oneshot:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -256,10 +256,10 @@ padlock_sha256_oneshot:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_sha256_oneshot:
 
-.globl	padlock_sha256_blocks
-.def	padlock_sha256_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_sha256_blocks
+.def	_gnutls_padlock_sha256_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_sha256_blocks:
+_gnutls_padlock_sha256_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -288,10 +288,10 @@ padlock_sha256_blocks:
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_sha256_blocks:
 
-.globl	padlock_sha512_blocks
-.def	padlock_sha512_blocks;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_sha512_blocks
+.def	_gnutls_padlock_sha512_blocks;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_sha512_blocks:
+_gnutls_padlock_sha512_blocks:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -326,10 +326,10 @@ padlock_sha512_blocks:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_sha512_blocks:
-.globl	padlock_ecb_encrypt
-.def	padlock_ecb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_ecb_encrypt
+.def	_gnutls_padlock_ecb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_ecb_encrypt:
+_gnutls_padlock_ecb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -504,10 +504,10 @@ padlock_ecb_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_ecb_encrypt:
-.globl	padlock_cbc_encrypt
-.def	padlock_cbc_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_cbc_encrypt
+.def	_gnutls_padlock_cbc_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_cbc_encrypt:
+_gnutls_padlock_cbc_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -686,10 +686,10 @@ padlock_cbc_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_cbc_encrypt:
-.globl	padlock_cfb_encrypt
-.def	padlock_cfb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_cfb_encrypt
+.def	_gnutls_padlock_cfb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_cfb_encrypt:
+_gnutls_padlock_cfb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -811,10 +811,10 @@ padlock_cfb_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_cfb_encrypt:
-.globl	padlock_ofb_encrypt
-.def	padlock_ofb_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_ofb_encrypt
+.def	_gnutls_padlock_ofb_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_ofb_encrypt:
+_gnutls_padlock_ofb_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -936,10 +936,10 @@ padlock_ofb_encrypt:
 	movq	16(%rsp),%rsi
 	.byte	0xf3,0xc3
 .LSEH_end_padlock_ofb_encrypt:
-.globl	padlock_ctr32_encrypt
-.def	padlock_ctr32_encrypt;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_padlock_ctr32_encrypt
+.def	_gnutls_padlock_ctr32_encrypt;	.scl 2;	.type 32;	.endef
 .p2align	4
-padlock_ctr32_encrypt:
+_gnutls_padlock_ctr32_encrypt:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/ghash-x86_64.s b/lib/accelerated/x86/coff/ghash-x86_64.s
index de207e4..b6f16b2 100644
--- a/lib/accelerated/x86/coff/ghash-x86_64.s
+++ b/lib/accelerated/x86/coff/ghash-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	gcm_gmult_4bit
-.def	gcm_gmult_4bit;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_gmult_4bit
+.def	_gnutls_gcm_gmult_4bit;	.scl 2;	.type 32;	.endef
 .p2align	4
-gcm_gmult_4bit:
+_gnutls_gcm_gmult_4bit:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -154,10 +154,10 @@ gcm_gmult_4bit:
 	.byte	0xf3,0xc3
 
 .LSEH_end_gcm_gmult_4bit:
-.globl	gcm_ghash_4bit
-.def	gcm_ghash_4bit;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_ghash_4bit
+.def	_gnutls_gcm_ghash_4bit;	.scl 2;	.type 32;	.endef
 .p2align	4
-gcm_ghash_4bit:
+_gnutls_gcm_ghash_4bit:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
@@ -748,10 +748,10 @@ gcm_ghash_4bit:
 	.byte	0xf3,0xc3
 
 .LSEH_end_gcm_ghash_4bit:
-.globl	gcm_init_clmul
-.def	gcm_init_clmul;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_init_clmul
+.def	_gnutls_gcm_init_clmul;	.scl 2;	.type 32;	.endef
 .p2align	4
-gcm_init_clmul:
+_gnutls_gcm_init_clmul:
 
 .L_init_clmul:
 .LSEH_begin_gcm_init_clmul:
@@ -913,10 +913,10 @@ gcm_init_clmul:
 	.byte	0xf3,0xc3
 
 
-.globl	gcm_gmult_clmul
-.def	gcm_gmult_clmul;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_gmult_clmul
+.def	_gnutls_gcm_gmult_clmul;	.scl 2;	.type 32;	.endef
 .p2align	4
-gcm_gmult_clmul:
+_gnutls_gcm_gmult_clmul:
 
 .L_gmult_clmul:
 	movdqu	(%rcx),%xmm0
@@ -966,10 +966,10 @@ gcm_gmult_clmul:
 	.byte	0xf3,0xc3
 
 
-.globl	gcm_ghash_clmul
-.def	gcm_ghash_clmul;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_ghash_clmul
+.def	_gnutls_gcm_ghash_clmul;	.scl 2;	.type 32;	.endef
 .p2align	5
-gcm_ghash_clmul:
+_gnutls_gcm_ghash_clmul:
 
 .L_ghash_clmul:
 	leaq	-136(%rsp),%rax
@@ -1377,10 +1377,10 @@ gcm_ghash_clmul:
 	.byte	0xf3,0xc3
 
 
-.globl	gcm_init_avx
-.def	gcm_init_avx;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_init_avx
+.def	_gnutls_gcm_init_avx;	.scl 2;	.type 32;	.endef
 .p2align	5
-gcm_init_avx:
+_gnutls_gcm_init_avx:
 
 .LSEH_begin_gcm_init_avx:
 
@@ -1493,18 +1493,18 @@ gcm_init_avx:
 	.byte	0xf3,0xc3
 
 
-.globl	gcm_gmult_avx
-.def	gcm_gmult_avx;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_gmult_avx
+.def	_gnutls_gcm_gmult_avx;	.scl 2;	.type 32;	.endef
 .p2align	5
-gcm_gmult_avx:
+_gnutls_gcm_gmult_avx:
 
 	jmp	.L_gmult_clmul
 
 
-.globl	gcm_ghash_avx
-.def	gcm_ghash_avx;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_gcm_ghash_avx
+.def	_gnutls_gcm_ghash_avx;	.scl 2;	.type 32;	.endef
 .p2align	5
-gcm_ghash_avx:
+_gnutls_gcm_ghash_avx:
 
 	leaq	-136(%rsp),%rax
 .LSEH_begin_gcm_ghash_avx:
diff --git a/lib/accelerated/x86/coff/sha1-ssse3-x86.s b/lib/accelerated/x86/coff/sha1-ssse3-x86.s
index 30f9ded..d594410 100644
--- a/lib/accelerated/x86/coff/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/sha1-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha1_block_data_order
-.def	_sha1_block_data_order;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_sha1_block_data_order
+.def	__gnutls_sha1_block_data_order;	.scl	2;	.type	32;	.endef
 .align	16
-_sha1_block_data_order:
+__gnutls_sha1_block_data_order:
 .L_sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s b/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
index cdfc882..7c68d86 100644
--- a/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha1_block_data_order
-.def	sha1_block_data_order;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_sha1_block_data_order
+.def	_gnutls_sha1_block_data_order;	.scl 2;	.type 32;	.endef
 .p2align	4
-sha1_block_data_order:
+_gnutls_sha1_block_data_order:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/sha256-ssse3-x86.s b/lib/accelerated/x86/coff/sha256-ssse3-x86.s
index 05cd61d..58daf37 100644
--- a/lib/accelerated/x86/coff/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/sha256-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha256_block_data_order
-.def	_sha256_block_data_order;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_sha256_block_data_order
+.def	__gnutls_sha256_block_data_order;	.scl	2;	.type	32;	.endef
 .align	16
-_sha256_block_data_order:
+__gnutls_sha256_block_data_order:
 .L_sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/coff/sha256-ssse3-x86_64.s b/lib/accelerated/x86/coff/sha256-ssse3-x86_64.s
index d2fc195..bc1e428 100644
--- a/lib/accelerated/x86/coff/sha256-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/sha256-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha256_block_data_order
-.def	sha256_block_data_order;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_sha256_block_data_order
+.def	_gnutls_sha256_block_data_order;	.scl 2;	.type 32;	.endef
 .p2align	4
-sha256_block_data_order:
+_gnutls_sha256_block_data_order:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/coff/sha512-ssse3-x86.s b/lib/accelerated/x86/coff/sha512-ssse3-x86.s
index 72a7f73..32c757a 100644
--- a/lib/accelerated/x86/coff/sha512-ssse3-x86.s
+++ b/lib/accelerated/x86/coff/sha512-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha512_block_data_order
-.def	_sha512_block_data_order;	.scl	2;	.type	32;	.endef
+.globl	__gnutls_sha512_block_data_order
+.def	__gnutls_sha512_block_data_order;	.scl	2;	.type	32;	.endef
 .align	16
-_sha512_block_data_order:
+__gnutls_sha512_block_data_order:
 .L_sha512_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s b/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
index 419fa2a..ed69f79 100644
--- a/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/coff/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha512_block_data_order
-.def	sha512_block_data_order;	.scl 2;	.type 32;	.endef
+.globl	_gnutls_sha512_block_data_order
+.def	_gnutls_sha512_block_data_order;	.scl 2;	.type 32;	.endef
 .p2align	4
-sha512_block_data_order:
+_gnutls_sha512_block_data_order:
 	movq	%rdi,8(%rsp)
 	movq	%rsi,16(%rsp)
 	movq	%rsp,%rax
diff --git a/lib/accelerated/x86/elf/aes-ssse3-x86.s b/lib/accelerated/x86/elf/aes-ssse3-x86.s
index 265e28a..8656799 100644
--- a/lib/accelerated/x86/elf/aes-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/aes-ssse3-x86.s
@@ -470,10 +470,10 @@ _vpaes_schedule_mangle:
 	movdqu	%xmm3,(%edx)
 	ret
 .size	_vpaes_schedule_mangle,.-_vpaes_schedule_mangle
-.globl	vpaes_set_encrypt_key
-.type	vpaes_set_encrypt_key,@function
+.globl	_gnutls_vpaes_set_encrypt_key
+.type	_gnutls_vpaes_set_encrypt_key,@function
 .align	16
-vpaes_set_encrypt_key:
+_gnutls_vpaes_set_encrypt_key:
 .L_vpaes_set_encrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -502,11 +502,11 @@ vpaes_set_encrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	vpaes_set_encrypt_key,.-.L_vpaes_set_encrypt_key_begin
-.globl	vpaes_set_decrypt_key
-.type	vpaes_set_decrypt_key,@function
+.size	_gnutls_vpaes_set_encrypt_key,.-.L_vpaes_set_encrypt_key_begin
+.globl	_gnutls_vpaes_set_decrypt_key
+.type	_gnutls_vpaes_set_decrypt_key,@function
 .align	16
-vpaes_set_decrypt_key:
+_gnutls_vpaes_set_decrypt_key:
 .L_vpaes_set_decrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -540,11 +540,11 @@ vpaes_set_decrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	vpaes_set_decrypt_key,.-.L_vpaes_set_decrypt_key_begin
-.globl	vpaes_encrypt
-.type	vpaes_encrypt,@function
+.size	_gnutls_vpaes_set_decrypt_key,.-.L_vpaes_set_decrypt_key_begin
+.globl	_gnutls_vpaes_encrypt
+.type	_gnutls_vpaes_encrypt,@function
 .align	16
-vpaes_encrypt:
+_gnutls_vpaes_encrypt:
 .L_vpaes_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -569,11 +569,11 @@ vpaes_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	vpaes_encrypt,.-.L_vpaes_encrypt_begin
-.globl	vpaes_decrypt
-.type	vpaes_decrypt,@function
+.size	_gnutls_vpaes_encrypt,.-.L_vpaes_encrypt_begin
+.globl	_gnutls_vpaes_decrypt
+.type	_gnutls_vpaes_decrypt,@function
 .align	16
-vpaes_decrypt:
+_gnutls_vpaes_decrypt:
 .L_vpaes_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -598,11 +598,11 @@ vpaes_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	vpaes_decrypt,.-.L_vpaes_decrypt_begin
-.globl	vpaes_cbc_encrypt
-.type	vpaes_cbc_encrypt,@function
+.size	_gnutls_vpaes_decrypt,.-.L_vpaes_decrypt_begin
+.globl	_gnutls_vpaes_cbc_encrypt
+.type	_gnutls_vpaes_cbc_encrypt,@function
 .align	16
-vpaes_cbc_encrypt:
+_gnutls_vpaes_cbc_encrypt:
 .L_vpaes_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -669,6 +669,6 @@ vpaes_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	vpaes_cbc_encrypt,.-.L_vpaes_cbc_encrypt_begin
+.size	_gnutls_vpaes_cbc_encrypt,.-.L_vpaes_cbc_encrypt_begin
 
 .section .note.GNU-stack,"",%progbits
diff --git a/lib/accelerated/x86/elf/aes-ssse3-x86_64.s b/lib/accelerated/x86/elf/aes-ssse3-x86_64.s
index ea1216b..a1f6309 100644
--- a/lib/accelerated/x86/elf/aes-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/aes-ssse3-x86_64.s
@@ -630,10 +630,10 @@ _vpaes_schedule_mangle:
 
 
 
-.globl	vpaes_set_encrypt_key
-.type	vpaes_set_encrypt_key,@function
+.globl	_gnutls_vpaes_set_encrypt_key
+.type	_gnutls_vpaes_set_encrypt_key,@function
 .align	16
-vpaes_set_encrypt_key:
+_gnutls_vpaes_set_encrypt_key:
 .cfi_startproc	
 	movl	%esi,%eax
 	shrl	$5,%eax
@@ -646,12 +646,12 @@ vpaes_set_encrypt_key:
 	xorl	%eax,%eax
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	vpaes_set_encrypt_key,.-vpaes_set_encrypt_key
+.size	_gnutls_vpaes_set_encrypt_key,.-_gnutls_vpaes_set_encrypt_key
 
-.globl	vpaes_set_decrypt_key
-.type	vpaes_set_decrypt_key,@function
+.globl	_gnutls_vpaes_set_decrypt_key
+.type	_gnutls_vpaes_set_decrypt_key,@function
 .align	16
-vpaes_set_decrypt_key:
+_gnutls_vpaes_set_decrypt_key:
 .cfi_startproc	
 	movl	%esi,%eax
 	shrl	$5,%eax
@@ -669,12 +669,12 @@ vpaes_set_decrypt_key:
 	xorl	%eax,%eax
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	vpaes_set_decrypt_key,.-vpaes_set_decrypt_key
+.size	_gnutls_vpaes_set_decrypt_key,.-_gnutls_vpaes_set_decrypt_key
 
-.globl	vpaes_encrypt
-.type	vpaes_encrypt,@function
+.globl	_gnutls_vpaes_encrypt
+.type	_gnutls_vpaes_encrypt,@function
 .align	16
-vpaes_encrypt:
+_gnutls_vpaes_encrypt:
 .cfi_startproc	
 	movdqu	(%rdi),%xmm0
 	call	_vpaes_preheat
@@ -682,12 +682,12 @@ vpaes_encrypt:
 	movdqu	%xmm0,(%rsi)
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	vpaes_encrypt,.-vpaes_encrypt
+.size	_gnutls_vpaes_encrypt,.-_gnutls_vpaes_encrypt
 
-.globl	vpaes_decrypt
-.type	vpaes_decrypt,@function
+.globl	_gnutls_vpaes_decrypt
+.type	_gnutls_vpaes_decrypt,@function
 .align	16
-vpaes_decrypt:
+_gnutls_vpaes_decrypt:
 .cfi_startproc	
 	movdqu	(%rdi),%xmm0
 	call	_vpaes_preheat
@@ -695,11 +695,11 @@ vpaes_decrypt:
 	movdqu	%xmm0,(%rsi)
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	vpaes_decrypt,.-vpaes_decrypt
-.globl	vpaes_cbc_encrypt
-.type	vpaes_cbc_encrypt,@function
+.size	_gnutls_vpaes_decrypt,.-_gnutls_vpaes_decrypt
+.globl	_gnutls_vpaes_cbc_encrypt
+.type	_gnutls_vpaes_cbc_encrypt,@function
 .align	16
-vpaes_cbc_encrypt:
+_gnutls_vpaes_cbc_encrypt:
 .cfi_startproc	
 	xchgq	%rcx,%rdx
 	subq	$16,%rcx
@@ -737,7 +737,7 @@ vpaes_cbc_encrypt:
 .Lcbc_abort:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	vpaes_cbc_encrypt,.-vpaes_cbc_encrypt
+.size	_gnutls_vpaes_cbc_encrypt,.-_gnutls_vpaes_cbc_encrypt
 
 
 
diff --git a/lib/accelerated/x86/elf/aesni-gcm-x86_64.s b/lib/accelerated/x86/elf/aesni-gcm-x86_64.s
index e26d18d..be1e916 100644
--- a/lib/accelerated/x86/elf/aesni-gcm-x86_64.s
+++ b/lib/accelerated/x86/elf/aesni-gcm-x86_64.s
@@ -350,10 +350,10 @@ _aesni_ctr32_ghash_6x:
 
 	.byte	0xf3,0xc3
 .size	_aesni_ctr32_ghash_6x,.-_aesni_ctr32_ghash_6x
-.globl	aesni_gcm_decrypt
-.type	aesni_gcm_decrypt,@function
+.globl	_gnutls_aesni_gcm_decrypt
+.type	_gnutls_aesni_gcm_decrypt,@function
 .align	32
-aesni_gcm_decrypt:
+_gnutls_aesni_gcm_decrypt:
 .cfi_startproc	
 	xorq	%r10,%r10
 	cmpq	$0x60,%rdx
@@ -451,7 +451,7 @@ aesni_gcm_decrypt:
 	movq	%r10,%rax
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_gcm_decrypt,.-aesni_gcm_decrypt
+.size	_gnutls_aesni_gcm_decrypt,.-_gnutls_aesni_gcm_decrypt
 .type	_aesni_ctr32_6x,@function
 .align	32
 _aesni_ctr32_6x:
@@ -543,10 +543,10 @@ _aesni_ctr32_6x:
 	jmp	.Loop_ctr32
 .size	_aesni_ctr32_6x,.-_aesni_ctr32_6x
 
-.globl	aesni_gcm_encrypt
-.type	aesni_gcm_encrypt,@function
+.globl	_gnutls_aesni_gcm_encrypt
+.type	_gnutls_aesni_gcm_encrypt,@function
 .align	32
-aesni_gcm_encrypt:
+_gnutls_aesni_gcm_encrypt:
 .cfi_startproc	
 	xorq	%r10,%r10
 	cmpq	$288,%rdx
@@ -808,7 +808,7 @@ aesni_gcm_encrypt:
 	movq	%r10,%rax
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_gcm_encrypt,.-aesni_gcm_encrypt
+.size	_gnutls_aesni_gcm_encrypt,.-_gnutls_aesni_gcm_encrypt
 .align	64
 .Lbswap_mask:
 .byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
diff --git a/lib/accelerated/x86/elf/aesni-x86.s b/lib/accelerated/x86/elf/aesni-x86.s
index aaf0bab..1dc9c00 100644
--- a/lib/accelerated/x86/elf/aesni-x86.s
+++ b/lib/accelerated/x86/elf/aesni-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	aesni_encrypt
-.type	aesni_encrypt,@function
+.globl	_gnutls_aesni_encrypt
+.type	_gnutls_aesni_encrypt,@function
 .align	16
-aesni_encrypt:
+_gnutls_aesni_encrypt:
 .L_aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -64,11 +64,11 @@ aesni_encrypt:
 	movups	%xmm2,(%eax)
 	pxor	%xmm2,%xmm2
 	ret
-.size	aesni_encrypt,.-.L_aesni_encrypt_begin
-.globl	aesni_decrypt
-.type	aesni_decrypt,@function
+.size	_gnutls_aesni_encrypt,.-.L_aesni_encrypt_begin
+.globl	_gnutls_aesni_decrypt
+.type	_gnutls_aesni_decrypt,@function
 .align	16
-aesni_decrypt:
+_gnutls_aesni_decrypt:
 .L_aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -91,7 +91,7 @@ aesni_decrypt:
 	movups	%xmm2,(%eax)
 	pxor	%xmm2,%xmm2
 	ret
-.size	aesni_decrypt,.-.L_aesni_decrypt_begin
+.size	_gnutls_aesni_decrypt,.-.L_aesni_decrypt_begin
 .type	_aesni_encrypt2,@function
 .align	16
 _aesni_encrypt2:
@@ -394,10 +394,10 @@ _aesni_decrypt6:
 .byte	102,15,56,223,248
 	ret
 .size	_aesni_decrypt6,.-_aesni_decrypt6
-.globl	aesni_ecb_encrypt
-.type	aesni_ecb_encrypt,@function
+.globl	_gnutls_aesni_ecb_encrypt
+.type	_gnutls_aesni_ecb_encrypt,@function
 .align	16
-aesni_ecb_encrypt:
+_gnutls_aesni_ecb_encrypt:
 .L_aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -628,11 +628,11 @@ aesni_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ecb_encrypt,.-.L_aesni_ecb_encrypt_begin
-.globl	aesni_ccm64_encrypt_blocks
-.type	aesni_ccm64_encrypt_blocks,@function
+.size	_gnutls_aesni_ecb_encrypt,.-.L_aesni_ecb_encrypt_begin
+.globl	_gnutls_aesni_ccm64_encrypt_blocks
+.type	_gnutls_aesni_ccm64_encrypt_blocks,@function
 .align	16
-aesni_ccm64_encrypt_blocks:
+_gnutls_aesni_ccm64_encrypt_blocks:
 .L_aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -716,11 +716,11 @@ aesni_ccm64_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ccm64_encrypt_blocks,.-.L_aesni_ccm64_encrypt_blocks_begin
-.globl	aesni_ccm64_decrypt_blocks
-.type	aesni_ccm64_decrypt_blocks,@function
+.size	_gnutls_aesni_ccm64_encrypt_blocks,.-.L_aesni_ccm64_encrypt_blocks_begin
+.globl	_gnutls_aesni_ccm64_decrypt_blocks
+.type	_gnutls_aesni_ccm64_decrypt_blocks,@function
 .align	16
-aesni_ccm64_decrypt_blocks:
+_gnutls_aesni_ccm64_decrypt_blocks:
 .L_aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -839,11 +839,11 @@ aesni_ccm64_decrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ccm64_decrypt_blocks,.-.L_aesni_ccm64_decrypt_blocks_begin
-.globl	aesni_ctr32_encrypt_blocks
-.type	aesni_ctr32_encrypt_blocks,@function
+.size	_gnutls_aesni_ccm64_decrypt_blocks,.-.L_aesni_ccm64_decrypt_blocks_begin
+.globl	_gnutls_aesni_ctr32_encrypt_blocks
+.type	_gnutls_aesni_ctr32_encrypt_blocks,@function
 .align	16
-aesni_ctr32_encrypt_blocks:
+_gnutls_aesni_ctr32_encrypt_blocks:
 .L_aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1077,11 +1077,11 @@ aesni_ctr32_encrypt_blocks:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ctr32_encrypt_blocks,.-.L_aesni_ctr32_encrypt_blocks_begin
-.globl	aesni_xts_encrypt
-.type	aesni_xts_encrypt,@function
+.size	_gnutls_aesni_ctr32_encrypt_blocks,.-.L_aesni_ctr32_encrypt_blocks_begin
+.globl	_gnutls_aesni_xts_encrypt
+.type	_gnutls_aesni_xts_encrypt,@function
 .align	16
-aesni_xts_encrypt:
+_gnutls_aesni_xts_encrypt:
 .L_aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1437,11 +1437,11 @@ aesni_xts_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_xts_encrypt,.-.L_aesni_xts_encrypt_begin
-.globl	aesni_xts_decrypt
-.type	aesni_xts_decrypt,@function
+.size	_gnutls_aesni_xts_encrypt,.-.L_aesni_xts_encrypt_begin
+.globl	_gnutls_aesni_xts_decrypt
+.type	_gnutls_aesni_xts_decrypt,@function
 .align	16
-aesni_xts_decrypt:
+_gnutls_aesni_xts_decrypt:
 .L_aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1827,11 +1827,11 @@ aesni_xts_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_xts_decrypt,.-.L_aesni_xts_decrypt_begin
-.globl	aesni_ocb_encrypt
-.type	aesni_ocb_encrypt,@function
+.size	_gnutls_aesni_xts_decrypt,.-.L_aesni_xts_decrypt_begin
+.globl	_gnutls_aesni_ocb_encrypt
+.type	_gnutls_aesni_ocb_encrypt,@function
 .align	16
-aesni_ocb_encrypt:
+_gnutls_aesni_ocb_encrypt:
 .L_aesni_ocb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2222,11 +2222,11 @@ aesni_ocb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ocb_encrypt,.-.L_aesni_ocb_encrypt_begin
-.globl	aesni_ocb_decrypt
-.type	aesni_ocb_decrypt,@function
+.size	_gnutls_aesni_ocb_encrypt,.-.L_aesni_ocb_encrypt_begin
+.globl	_gnutls_aesni_ocb_decrypt
+.type	_gnutls_aesni_ocb_decrypt,@function
 .align	16
-aesni_ocb_decrypt:
+_gnutls_aesni_ocb_decrypt:
 .L_aesni_ocb_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2617,11 +2617,11 @@ aesni_ocb_decrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_ocb_decrypt,.-.L_aesni_ocb_decrypt_begin
-.globl	aesni_cbc_encrypt
-.type	aesni_cbc_encrypt,@function
+.size	_gnutls_aesni_ocb_decrypt,.-.L_aesni_ocb_decrypt_begin
+.globl	_gnutls_aesni_cbc_encrypt
+.type	_gnutls_aesni_cbc_encrypt,@function
 .align	16
-aesni_cbc_encrypt:
+_gnutls_aesni_cbc_encrypt:
 .L_aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2878,10 +2878,10 @@ aesni_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	aesni_cbc_encrypt,.-.L_aesni_cbc_encrypt_begin
-.type	_aesni_set_encrypt_key,@function
+.size	_gnutls_aesni_cbc_encrypt,.-.L_aesni_cbc_encrypt_begin
+.type	__gnutls_aesni_set_encrypt_key,@function
 .align	16
-_aesni_set_encrypt_key:
+__gnutls_aesni_set_encrypt_key:
 	pushl	%ebp
 	pushl	%ebx
 	testl	%eax,%eax
@@ -3211,27 +3211,27 @@ _aesni_set_encrypt_key:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	_aesni_set_encrypt_key,.-_aesni_set_encrypt_key
-.globl	aesni_set_encrypt_key
-.type	aesni_set_encrypt_key,@function
+.size	__gnutls_aesni_set_encrypt_key,.-__gnutls_aesni_set_encrypt_key
+.globl	_gnutls_aesni_set_encrypt_key
+.type	_gnutls_aesni_set_encrypt_key,@function
 .align	16
-aesni_set_encrypt_key:
+_gnutls_aesni_set_encrypt_key:
 .L_aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	_aesni_set_encrypt_key
+	call	__gnutls_aesni_set_encrypt_key
 	ret
-.size	aesni_set_encrypt_key,.-.L_aesni_set_encrypt_key_begin
-.globl	aesni_set_decrypt_key
-.type	aesni_set_decrypt_key,@function
+.size	_gnutls_aesni_set_encrypt_key,.-.L_aesni_set_encrypt_key_begin
+.globl	_gnutls_aesni_set_decrypt_key
+.type	_gnutls_aesni_set_decrypt_key,@function
 .align	16
-aesni_set_decrypt_key:
+_gnutls_aesni_set_decrypt_key:
 .L_aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
-	call	_aesni_set_encrypt_key
+	call	__gnutls_aesni_set_encrypt_key
 	movl	12(%esp),%edx
 	shll	$4,%ecx
 	testl	%eax,%eax
@@ -3262,7 +3262,7 @@ aesni_set_decrypt_key:
 	xorl	%eax,%eax
 .L134dec_key_ret:
 	ret
-.size	aesni_set_decrypt_key,.-.L_aesni_set_decrypt_key_begin
+.size	_gnutls_aesni_set_decrypt_key,.-.L_aesni_set_decrypt_key_begin
 .align	64
 .Lkey_const:
 .long	202313229,202313229,202313229,202313229
diff --git a/lib/accelerated/x86/elf/aesni-x86_64.s b/lib/accelerated/x86/elf/aesni-x86_64.s
index 43cf4e6..6c75c96 100644
--- a/lib/accelerated/x86/elf/aesni-x86_64.s
+++ b/lib/accelerated/x86/elf/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	aesni_encrypt
-.type	aesni_encrypt,@function
+.globl	_gnutls_aesni_encrypt
+.type	_gnutls_aesni_encrypt,@function
 .align	16
-aesni_encrypt:
+_gnutls_aesni_encrypt:
 .cfi_startproc	
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
@@ -63,12 +63,12 @@ aesni_encrypt:
 	pxor	%xmm2,%xmm2
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_encrypt,.-aesni_encrypt
+.size	_gnutls_aesni_encrypt,.-_gnutls_aesni_encrypt
 
-.globl	aesni_decrypt
-.type	aesni_decrypt,@function
+.globl	_gnutls_aesni_decrypt
+.type	_gnutls_aesni_decrypt,@function
 .align	16
-aesni_decrypt:
+_gnutls_aesni_decrypt:
 .cfi_startproc	
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
@@ -89,7 +89,7 @@ aesni_decrypt:
 	pxor	%xmm2,%xmm2
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_decrypt, .-aesni_decrypt
+.size	_gnutls_aesni_decrypt, .-_gnutls_aesni_decrypt
 .type	_aesni_encrypt2,@function
 .align	16
 _aesni_encrypt2:
@@ -552,10 +552,10 @@ _aesni_decrypt8:
 	.byte	0xf3,0xc3
 .cfi_endproc	
 .size	_aesni_decrypt8,.-_aesni_decrypt8
-.globl	aesni_ecb_encrypt
-.type	aesni_ecb_encrypt,@function
+.globl	_gnutls_aesni_ecb_encrypt
+.type	_gnutls_aesni_ecb_encrypt,@function
 .align	16
-aesni_ecb_encrypt:
+_gnutls_aesni_ecb_encrypt:
 .cfi_startproc	
 	andq	$-16,%rdx
 	jz	.Lecb_ret
@@ -895,11 +895,11 @@ aesni_ecb_encrypt:
 	pxor	%xmm1,%xmm1
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_ecb_encrypt,.-aesni_ecb_encrypt
-.globl	aesni_ccm64_encrypt_blocks
-.type	aesni_ccm64_encrypt_blocks,@function
+.size	_gnutls_aesni_ecb_encrypt,.-_gnutls_aesni_ecb_encrypt
+.globl	_gnutls_aesni_ccm64_encrypt_blocks
+.type	_gnutls_aesni_ccm64_encrypt_blocks,@function
 .align	16
-aesni_ccm64_encrypt_blocks:
+_gnutls_aesni_ccm64_encrypt_blocks:
 	movl	240(%rcx),%eax
 	movdqu	(%r8),%xmm6
 	movdqa	.Lincrement64(%rip),%xmm9
@@ -958,11 +958,11 @@ aesni_ccm64_encrypt_blocks:
 	pxor	%xmm8,%xmm8
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
-.size	aesni_ccm64_encrypt_blocks,.-aesni_ccm64_encrypt_blocks
-.globl	aesni_ccm64_decrypt_blocks
-.type	aesni_ccm64_decrypt_blocks,@function
+.size	_gnutls_aesni_ccm64_encrypt_blocks,.-_gnutls_aesni_ccm64_encrypt_blocks
+.globl	_gnutls_aesni_ccm64_decrypt_blocks
+.type	_gnutls_aesni_ccm64_decrypt_blocks,@function
 .align	16
-aesni_ccm64_decrypt_blocks:
+_gnutls_aesni_ccm64_decrypt_blocks:
 	movl	240(%rcx),%eax
 	movups	(%r8),%xmm6
 	movdqu	(%r9),%xmm3
@@ -1055,11 +1055,11 @@ aesni_ccm64_decrypt_blocks:
 	pxor	%xmm8,%xmm8
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
-.size	aesni_ccm64_decrypt_blocks,.-aesni_ccm64_decrypt_blocks
-.globl	aesni_ctr32_encrypt_blocks
-.type	aesni_ctr32_encrypt_blocks,@function
+.size	_gnutls_aesni_ccm64_decrypt_blocks,.-_gnutls_aesni_ccm64_decrypt_blocks
+.globl	_gnutls_aesni_ctr32_encrypt_blocks
+.type	_gnutls_aesni_ctr32_encrypt_blocks,@function
 .align	16
-aesni_ctr32_encrypt_blocks:
+_gnutls_aesni_ctr32_encrypt_blocks:
 .cfi_startproc	
 	cmpq	$1,%rdx
 	jne	.Lctr32_bulk
@@ -1633,11 +1633,11 @@ aesni_ctr32_encrypt_blocks:
 .Lctr32_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_ctr32_encrypt_blocks,.-aesni_ctr32_encrypt_blocks
-.globl	aesni_xts_encrypt
-.type	aesni_xts_encrypt,@function
+.size	_gnutls_aesni_ctr32_encrypt_blocks,.-_gnutls_aesni_ctr32_encrypt_blocks
+.globl	_gnutls_aesni_xts_encrypt
+.type	_gnutls_aesni_xts_encrypt,@function
 .align	16
-aesni_xts_encrypt:
+_gnutls_aesni_xts_encrypt:
 .cfi_startproc	
 	leaq	(%rsp),%r11
 .cfi_def_cfa_register	%r11
@@ -2103,11 +2103,11 @@ aesni_xts_encrypt:
 .Lxts_enc_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_xts_encrypt,.-aesni_xts_encrypt
-.globl	aesni_xts_decrypt
-.type	aesni_xts_decrypt,@function
+.size	_gnutls_aesni_xts_encrypt,.-_gnutls_aesni_xts_encrypt
+.globl	_gnutls_aesni_xts_decrypt
+.type	_gnutls_aesni_xts_decrypt,@function
 .align	16
-aesni_xts_decrypt:
+_gnutls_aesni_xts_decrypt:
 .cfi_startproc	
 	leaq	(%rsp),%r11
 .cfi_def_cfa_register	%r11
@@ -2610,11 +2610,11 @@ aesni_xts_decrypt:
 .Lxts_dec_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_xts_decrypt,.-aesni_xts_decrypt
-.globl	aesni_ocb_encrypt
-.type	aesni_ocb_encrypt,@function
+.size	_gnutls_aesni_xts_decrypt,.-_gnutls_aesni_xts_decrypt
+.globl	_gnutls_aesni_ocb_encrypt
+.type	_gnutls_aesni_ocb_encrypt,@function
 .align	32
-aesni_ocb_encrypt:
+_gnutls_aesni_ocb_encrypt:
 .cfi_startproc	
 	leaq	(%rsp),%rax
 	pushq	%rbx
@@ -2824,7 +2824,7 @@ aesni_ocb_encrypt:
 .Locb_enc_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_ocb_encrypt,.-aesni_ocb_encrypt
+.size	_gnutls_aesni_ocb_encrypt,.-_gnutls_aesni_ocb_encrypt
 
 .type	__ocb_encrypt6,@function
 .align	32
@@ -3032,10 +3032,10 @@ __ocb_encrypt1:
 	.byte	0xf3,0xc3
 .size	__ocb_encrypt1,.-__ocb_encrypt1
 
-.globl	aesni_ocb_decrypt
-.type	aesni_ocb_decrypt,@function
+.globl	_gnutls_aesni_ocb_decrypt
+.type	_gnutls_aesni_ocb_decrypt,@function
 .align	32
-aesni_ocb_decrypt:
+_gnutls_aesni_ocb_decrypt:
 .cfi_startproc	
 	leaq	(%rsp),%rax
 	pushq	%rbx
@@ -3267,7 +3267,7 @@ aesni_ocb_decrypt:
 .Locb_dec_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_ocb_decrypt,.-aesni_ocb_decrypt
+.size	_gnutls_aesni_ocb_decrypt,.-_gnutls_aesni_ocb_decrypt
 
 .type	__ocb_decrypt6,@function
 .align	32
@@ -3463,10 +3463,10 @@ __ocb_decrypt1:
 .byte	102,15,56,223,215
 	.byte	0xf3,0xc3
 .size	__ocb_decrypt1,.-__ocb_decrypt1
-.globl	aesni_cbc_encrypt
-.type	aesni_cbc_encrypt,@function
+.globl	_gnutls_aesni_cbc_encrypt
+.type	_gnutls_aesni_cbc_encrypt,@function
 .align	16
-aesni_cbc_encrypt:
+_gnutls_aesni_cbc_encrypt:
 .cfi_startproc	
 	testq	%rdx,%rdx
 	jz	.Lcbc_ret
@@ -4053,11 +4053,11 @@ aesni_cbc_encrypt:
 .Lcbc_ret:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	aesni_cbc_encrypt,.-aesni_cbc_encrypt
-.globl	aesni_set_decrypt_key
-.type	aesni_set_decrypt_key,@function
+.size	_gnutls_aesni_cbc_encrypt,.-_gnutls_aesni_cbc_encrypt
+.globl	_gnutls_aesni_set_decrypt_key
+.type	_gnutls_aesni_set_decrypt_key,@function
 .align	16
-aesni_set_decrypt_key:
+_gnutls_aesni_set_decrypt_key:
 .cfi_startproc	
 .byte	0x48,0x83,0xEC,0x08
 .cfi_adjust_cfa_offset	8
@@ -4097,11 +4097,11 @@ aesni_set_decrypt_key:
 	.byte	0xf3,0xc3
 .cfi_endproc	
 .LSEH_end_set_decrypt_key:
-.size	aesni_set_decrypt_key,.-aesni_set_decrypt_key
-.globl	aesni_set_encrypt_key
-.type	aesni_set_encrypt_key,@function
+.size	_gnutls_aesni_set_decrypt_key,.-_gnutls_aesni_set_decrypt_key
+.globl	_gnutls_aesni_set_encrypt_key
+.type	_gnutls_aesni_set_encrypt_key,@function
 .align	16
-aesni_set_encrypt_key:
+_gnutls_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
 .cfi_startproc	
 .byte	0x48,0x83,0xEC,0x08
@@ -4471,7 +4471,7 @@ __aesni_set_encrypt_key:
 	shufps	$170,%xmm1,%xmm1
 	xorps	%xmm1,%xmm2
 	.byte	0xf3,0xc3
-.size	aesni_set_encrypt_key,.-aesni_set_encrypt_key
+.size	_gnutls_aesni_set_encrypt_key,.-_gnutls_aesni_set_encrypt_key
 .size	__aesni_set_encrypt_key,.-__aesni_set_encrypt_key
 .align	64
 .Lbswap_mask:
diff --git a/lib/accelerated/x86/elf/e_padlock-x86.s b/lib/accelerated/x86/elf/e_padlock-x86.s
index ed8681e..8239f8e 100644
--- a/lib/accelerated/x86/elf/e_padlock-x86.s
+++ b/lib/accelerated/x86/elf/e_padlock-x86.s
@@ -39,10 +39,10 @@
 #
 .file	"devel/perlasm/e_padlock-x86.s"
 .text
-.globl	padlock_capability
-.type	padlock_capability,@function
+.globl	_gnutls_padlock_capability
+.type	_gnutls_padlock_capability,@function
 .align	16
-padlock_capability:
+_gnutls_padlock_capability:
 .L_padlock_capability_begin:
 	pushl	%ebx
 	pushfl
@@ -89,11 +89,11 @@ padlock_capability:
 .L000noluck:
 	popl	%ebx
 	ret
-.size	padlock_capability,.-.L_padlock_capability_begin
-.globl	padlock_key_bswap
-.type	padlock_key_bswap,@function
+.size	_gnutls_padlock_capability,.-.L_padlock_capability_begin
+.globl	_gnutls_padlock_key_bswap
+.type	_gnutls_padlock_key_bswap,@function
 .align	16
-padlock_key_bswap:
+_gnutls_padlock_key_bswap:
 .L_padlock_key_bswap_begin:
 	movl	4(%esp),%edx
 	movl	240(%edx),%ecx
@@ -105,11 +105,11 @@ padlock_key_bswap:
 	subl	$1,%ecx
 	jnz	.L001bswap_loop
 	ret
-.size	padlock_key_bswap,.-.L_padlock_key_bswap_begin
-.globl	padlock_verify_context
-.type	padlock_verify_context,@function
+.size	_gnutls_padlock_key_bswap,.-.L_padlock_key_bswap_begin
+.globl	_gnutls_padlock_verify_context
+.type	_gnutls_padlock_verify_context,@function
 .align	16
-padlock_verify_context:
+_gnutls_padlock_verify_context:
 .L_padlock_verify_context_begin:
 	movl	4(%esp),%edx
 	leal	.Lpadlock_saved_context-.L002verify_pic_point,%eax
@@ -118,7 +118,7 @@ padlock_verify_context:
 .L002verify_pic_point:
 	leal	4(%esp),%esp
 	ret
-.size	padlock_verify_context,.-.L_padlock_verify_context_begin
+.size	_gnutls_padlock_verify_context,.-.L_padlock_verify_context_begin
 .type	_padlock_verify_ctx,@function
 .align	16
 _padlock_verify_ctx:
@@ -133,19 +133,19 @@ _padlock_verify_ctx:
 	movl	%edx,(%eax)
 	ret
 .size	_padlock_verify_ctx,.-_padlock_verify_ctx
-.globl	padlock_reload_key
-.type	padlock_reload_key,@function
+.globl	_gnutls_padlock_reload_key
+.type	_gnutls_padlock_reload_key,@function
 .align	16
-padlock_reload_key:
+_gnutls_padlock_reload_key:
 .L_padlock_reload_key_begin:
 	pushfl
 	popfl
 	ret
-.size	padlock_reload_key,.-.L_padlock_reload_key_begin
-.globl	padlock_aes_block
-.type	padlock_aes_block,@function
+.size	_gnutls_padlock_reload_key,.-.L_padlock_reload_key_begin
+.globl	_gnutls_padlock_aes_block
+.type	_gnutls_padlock_aes_block,@function
 .align	16
-padlock_aes_block:
+_gnutls_padlock_aes_block:
 .L_padlock_aes_block_begin:
 	pushl	%edi
 	pushl	%esi
@@ -161,11 +161,11 @@ padlock_aes_block:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_aes_block,.-.L_padlock_aes_block_begin
-.globl	padlock_ecb_encrypt
-.type	padlock_ecb_encrypt,@function
+.size	_gnutls_padlock_aes_block,.-.L_padlock_aes_block_begin
+.globl	_gnutls_padlock_ecb_encrypt
+.type	_gnutls_padlock_ecb_encrypt,@function
 .align	16
-padlock_ecb_encrypt:
+_gnutls_padlock_ecb_encrypt:
 .L_padlock_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -340,11 +340,11 @@ padlock_ecb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	padlock_ecb_encrypt,.-.L_padlock_ecb_encrypt_begin
-.globl	padlock_cbc_encrypt
-.type	padlock_cbc_encrypt,@function
+.size	_gnutls_padlock_ecb_encrypt,.-.L_padlock_ecb_encrypt_begin
+.globl	_gnutls_padlock_cbc_encrypt
+.type	_gnutls_padlock_cbc_encrypt,@function
 .align	16
-padlock_cbc_encrypt:
+_gnutls_padlock_cbc_encrypt:
 .L_padlock_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -523,11 +523,11 @@ padlock_cbc_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	padlock_cbc_encrypt,.-.L_padlock_cbc_encrypt_begin
-.globl	padlock_cfb_encrypt
-.type	padlock_cfb_encrypt,@function
+.size	_gnutls_padlock_cbc_encrypt,.-.L_padlock_cbc_encrypt_begin
+.globl	_gnutls_padlock_cfb_encrypt
+.type	_gnutls_padlock_cfb_encrypt,@function
 .align	16
-padlock_cfb_encrypt:
+_gnutls_padlock_cfb_encrypt:
 .L_padlock_cfb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -645,11 +645,11 @@ padlock_cfb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	padlock_cfb_encrypt,.-.L_padlock_cfb_encrypt_begin
-.globl	padlock_ofb_encrypt
-.type	padlock_ofb_encrypt,@function
+.size	_gnutls_padlock_cfb_encrypt,.-.L_padlock_cfb_encrypt_begin
+.globl	_gnutls_padlock_ofb_encrypt
+.type	_gnutls_padlock_ofb_encrypt,@function
 .align	16
-padlock_ofb_encrypt:
+_gnutls_padlock_ofb_encrypt:
 .L_padlock_ofb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -767,11 +767,11 @@ padlock_ofb_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	padlock_ofb_encrypt,.-.L_padlock_ofb_encrypt_begin
-.globl	padlock_ctr32_encrypt
-.type	padlock_ctr32_encrypt,@function
+.size	_gnutls_padlock_ofb_encrypt,.-.L_padlock_ofb_encrypt_begin
+.globl	_gnutls_padlock_ctr32_encrypt
+.type	_gnutls_padlock_ctr32_encrypt,@function
 .align	16
-padlock_ctr32_encrypt:
+_gnutls_padlock_ctr32_encrypt:
 .L_padlock_ctr32_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -874,11 +874,11 @@ padlock_ctr32_encrypt:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	padlock_ctr32_encrypt,.-.L_padlock_ctr32_encrypt_begin
-.globl	padlock_xstore
-.type	padlock_xstore,@function
+.size	_gnutls_padlock_ctr32_encrypt,.-.L_padlock_ctr32_encrypt_begin
+.globl	_gnutls_padlock_xstore
+.type	_gnutls_padlock_xstore,@function
 .align	16
-padlock_xstore:
+_gnutls_padlock_xstore:
 .L_padlock_xstore_begin:
 	pushl	%edi
 	movl	8(%esp),%edi
@@ -886,7 +886,7 @@ padlock_xstore:
 .byte	15,167,192
 	popl	%edi
 	ret
-.size	padlock_xstore,.-.L_padlock_xstore_begin
+.size	_gnutls_padlock_xstore,.-.L_padlock_xstore_begin
 .type	_win32_segv_handler,@function
 .align	16
 _win32_segv_handler:
@@ -900,10 +900,10 @@ _win32_segv_handler:
 .L053ret:
 	ret
 .size	_win32_segv_handler,.-_win32_segv_handler
-.globl	padlock_sha1_oneshot
-.type	padlock_sha1_oneshot,@function
+.globl	_gnutls_padlock_sha1_oneshot
+.type	_gnutls_padlock_sha1_oneshot,@function
 .align	16
-padlock_sha1_oneshot:
+_gnutls_padlock_sha1_oneshot:
 .L_padlock_sha1_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -930,11 +930,11 @@ padlock_sha1_oneshot:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_sha1_oneshot,.-.L_padlock_sha1_oneshot_begin
-.globl	padlock_sha1_blocks
-.type	padlock_sha1_blocks,@function
+.size	_gnutls_padlock_sha1_oneshot,.-.L_padlock_sha1_oneshot_begin
+.globl	_gnutls_padlock_sha1_blocks
+.type	_gnutls_padlock_sha1_blocks,@function
 .align	16
-padlock_sha1_blocks:
+_gnutls_padlock_sha1_blocks:
 .L_padlock_sha1_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -960,11 +960,11 @@ padlock_sha1_blocks:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_sha1_blocks,.-.L_padlock_sha1_blocks_begin
-.globl	padlock_sha256_oneshot
-.type	padlock_sha256_oneshot,@function
+.size	_gnutls_padlock_sha1_blocks,.-.L_padlock_sha1_blocks_begin
+.globl	_gnutls_padlock_sha256_oneshot
+.type	_gnutls_padlock_sha256_oneshot,@function
 .align	16
-padlock_sha256_oneshot:
+_gnutls_padlock_sha256_oneshot:
 .L_padlock_sha256_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -991,11 +991,11 @@ padlock_sha256_oneshot:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_sha256_oneshot,.-.L_padlock_sha256_oneshot_begin
-.globl	padlock_sha256_blocks
-.type	padlock_sha256_blocks,@function
+.size	_gnutls_padlock_sha256_oneshot,.-.L_padlock_sha256_oneshot_begin
+.globl	_gnutls_padlock_sha256_blocks
+.type	_gnutls_padlock_sha256_blocks,@function
 .align	16
-padlock_sha256_blocks:
+_gnutls_padlock_sha256_blocks:
 .L_padlock_sha256_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -1021,11 +1021,11 @@ padlock_sha256_blocks:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_sha256_blocks,.-.L_padlock_sha256_blocks_begin
-.globl	padlock_sha512_blocks
-.type	padlock_sha512_blocks,@function
+.size	_gnutls_padlock_sha256_blocks,.-.L_padlock_sha256_blocks_begin
+.globl	_gnutls_padlock_sha512_blocks
+.type	_gnutls_padlock_sha512_blocks,@function
 .align	16
-padlock_sha512_blocks:
+_gnutls_padlock_sha512_blocks:
 .L_padlock_sha512_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -1058,7 +1058,7 @@ padlock_sha512_blocks:
 	popl	%esi
 	popl	%edi
 	ret
-.size	padlock_sha512_blocks,.-.L_padlock_sha512_blocks_begin
+.size	_gnutls_padlock_sha512_blocks,.-.L_padlock_sha512_blocks_begin
 .byte	86,73,65,32,80,97,100,108,111,99,107,32,120,56,54,32
 .byte	109,111,100,117,108,101,44,32,67,82,89,80,84,79,71,65
 .byte	77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101
diff --git a/lib/accelerated/x86/elf/e_padlock-x86_64.s b/lib/accelerated/x86/elf/e_padlock-x86_64.s
index c161f0a..d3e4267 100644
--- a/lib/accelerated/x86/elf/e_padlock-x86_64.s
+++ b/lib/accelerated/x86/elf/e_padlock-x86_64.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text	
-.globl	padlock_capability
-.type	padlock_capability,@function
+.globl	_gnutls_padlock_capability
+.type	_gnutls_padlock_capability,@function
 .align	16
-padlock_capability:
+_gnutls_padlock_capability:
 	movq	%rbx,%r8
 	xorl	%eax,%eax
 	cpuid
@@ -66,12 +66,12 @@ padlock_capability:
 .Lnoluck:
 	movq	%r8,%rbx
 	.byte	0xf3,0xc3
-.size	padlock_capability,.-padlock_capability
+.size	_gnutls_padlock_capability,.-_gnutls_padlock_capability
 
-.globl	padlock_key_bswap
-.type	padlock_key_bswap,@function
+.globl	_gnutls_padlock_key_bswap
+.type	_gnutls_padlock_key_bswap,@function
 .align	16
-padlock_key_bswap:
+_gnutls_padlock_key_bswap:
 	movl	240(%rdi),%edx
 .Lbswap_loop:
 	movl	(%rdi),%eax
@@ -81,19 +81,19 @@ padlock_key_bswap:
 	subl	$1,%edx
 	jnz	.Lbswap_loop
 	.byte	0xf3,0xc3
-.size	padlock_key_bswap,.-padlock_key_bswap
+.size	_gnutls_padlock_key_bswap,.-_gnutls_padlock_key_bswap
 
-.globl	padlock_verify_context
-.type	padlock_verify_context,@function
+.globl	_gnutls_padlock_verify_context
+.type	_gnutls_padlock_verify_context,@function
 .align	16
-padlock_verify_context:
+_gnutls_padlock_verify_context:
 	movq	%rdi,%rdx
 	pushf
 	leaq	.Lpadlock_saved_context(%rip),%rax
 	call	_padlock_verify_ctx
 	leaq	8(%rsp),%rsp
 	.byte	0xf3,0xc3
-.size	padlock_verify_context,.-padlock_verify_context
+.size	_gnutls_padlock_verify_context,.-_gnutls_padlock_verify_context
 
 .type	_padlock_verify_ctx,@function
 .align	16
@@ -110,19 +110,19 @@ _padlock_verify_ctx:
 	.byte	0xf3,0xc3
 .size	_padlock_verify_ctx,.-_padlock_verify_ctx
 
-.globl	padlock_reload_key
-.type	padlock_reload_key,@function
+.globl	_gnutls_padlock_reload_key
+.type	_gnutls_padlock_reload_key,@function
 .align	16
-padlock_reload_key:
+_gnutls_padlock_reload_key:
 	pushf
 	popf
 	.byte	0xf3,0xc3
-.size	padlock_reload_key,.-padlock_reload_key
+.size	_gnutls_padlock_reload_key,.-_gnutls_padlock_reload_key
 
-.globl	padlock_aes_block
-.type	padlock_aes_block,@function
+.globl	_gnutls_padlock_aes_block
+.type	_gnutls_padlock_aes_block,@function
 .align	16
-padlock_aes_block:
+_gnutls_padlock_aes_block:
 	movq	%rbx,%r8
 	movq	$1,%rcx
 	leaq	32(%rdx),%rbx
@@ -130,21 +130,21 @@ padlock_aes_block:
 .byte	0xf3,0x0f,0xa7,0xc8	
 	movq	%r8,%rbx
 	.byte	0xf3,0xc3
-.size	padlock_aes_block,.-padlock_aes_block
+.size	_gnutls_padlock_aes_block,.-_gnutls_padlock_aes_block
 
-.globl	padlock_xstore
-.type	padlock_xstore,@function
+.globl	_gnutls_padlock_xstore
+.type	_gnutls_padlock_xstore,@function
 .align	16
-padlock_xstore:
+_gnutls_padlock_xstore:
 	movl	%esi,%edx
 .byte	0x0f,0xa7,0xc0		
 	.byte	0xf3,0xc3
-.size	padlock_xstore,.-padlock_xstore
+.size	_gnutls_padlock_xstore,.-_gnutls_padlock_xstore
 
-.globl	padlock_sha1_oneshot
-.type	padlock_sha1_oneshot,@function
+.globl	_gnutls_padlock_sha1_oneshot
+.type	_gnutls_padlock_sha1_oneshot,@function
 .align	16
-padlock_sha1_oneshot:
+_gnutls_padlock_sha1_oneshot:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -161,12 +161,12 @@ padlock_sha1_oneshot:
 	movups	%xmm0,(%rdx)
 	movl	%eax,16(%rdx)
 	.byte	0xf3,0xc3
-.size	padlock_sha1_oneshot,.-padlock_sha1_oneshot
+.size	_gnutls_padlock_sha1_oneshot,.-_gnutls_padlock_sha1_oneshot
 
-.globl	padlock_sha1_blocks
-.type	padlock_sha1_blocks,@function
+.globl	_gnutls_padlock_sha1_blocks
+.type	_gnutls_padlock_sha1_blocks,@function
 .align	16
-padlock_sha1_blocks:
+_gnutls_padlock_sha1_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -183,12 +183,12 @@ padlock_sha1_blocks:
 	movups	%xmm0,(%rdx)
 	movl	%eax,16(%rdx)
 	.byte	0xf3,0xc3
-.size	padlock_sha1_blocks,.-padlock_sha1_blocks
+.size	_gnutls_padlock_sha1_blocks,.-_gnutls_padlock_sha1_blocks
 
-.globl	padlock_sha256_oneshot
-.type	padlock_sha256_oneshot,@function
+.globl	_gnutls_padlock_sha256_oneshot
+.type	_gnutls_padlock_sha256_oneshot,@function
 .align	16
-padlock_sha256_oneshot:
+_gnutls_padlock_sha256_oneshot:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -205,12 +205,12 @@ padlock_sha256_oneshot:
 	movups	%xmm0,(%rdx)
 	movups	%xmm1,16(%rdx)
 	.byte	0xf3,0xc3
-.size	padlock_sha256_oneshot,.-padlock_sha256_oneshot
+.size	_gnutls_padlock_sha256_oneshot,.-_gnutls_padlock_sha256_oneshot
 
-.globl	padlock_sha256_blocks
-.type	padlock_sha256_blocks,@function
+.globl	_gnutls_padlock_sha256_blocks
+.type	_gnutls_padlock_sha256_blocks,@function
 .align	16
-padlock_sha256_blocks:
+_gnutls_padlock_sha256_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -227,12 +227,12 @@ padlock_sha256_blocks:
 	movups	%xmm0,(%rdx)
 	movups	%xmm1,16(%rdx)
 	.byte	0xf3,0xc3
-.size	padlock_sha256_blocks,.-padlock_sha256_blocks
+.size	_gnutls_padlock_sha256_blocks,.-_gnutls_padlock_sha256_blocks
 
-.globl	padlock_sha512_blocks
-.type	padlock_sha512_blocks,@function
+.globl	_gnutls_padlock_sha512_blocks
+.type	_gnutls_padlock_sha512_blocks,@function
 .align	16
-padlock_sha512_blocks:
+_gnutls_padlock_sha512_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -256,11 +256,11 @@ padlock_sha512_blocks:
 	movups	%xmm2,32(%rdx)
 	movups	%xmm3,48(%rdx)
 	.byte	0xf3,0xc3
-.size	padlock_sha512_blocks,.-padlock_sha512_blocks
-.globl	padlock_ecb_encrypt
-.type	padlock_ecb_encrypt,@function
+.size	_gnutls_padlock_sha512_blocks,.-_gnutls_padlock_sha512_blocks
+.globl	_gnutls_padlock_ecb_encrypt
+.type	_gnutls_padlock_ecb_encrypt,@function
 .align	16
-padlock_ecb_encrypt:
+_gnutls_padlock_ecb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -423,11 +423,11 @@ padlock_ecb_encrypt:
 	popq	%rbx
 	popq	%rbp
 	.byte	0xf3,0xc3
-.size	padlock_ecb_encrypt,.-padlock_ecb_encrypt
-.globl	padlock_cbc_encrypt
-.type	padlock_cbc_encrypt,@function
+.size	_gnutls_padlock_ecb_encrypt,.-_gnutls_padlock_ecb_encrypt
+.globl	_gnutls_padlock_cbc_encrypt
+.type	_gnutls_padlock_cbc_encrypt,@function
 .align	16
-padlock_cbc_encrypt:
+_gnutls_padlock_cbc_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -594,11 +594,11 @@ padlock_cbc_encrypt:
 	popq	%rbx
 	popq	%rbp
 	.byte	0xf3,0xc3
-.size	padlock_cbc_encrypt,.-padlock_cbc_encrypt
-.globl	padlock_cfb_encrypt
-.type	padlock_cfb_encrypt,@function
+.size	_gnutls_padlock_cbc_encrypt,.-_gnutls_padlock_cbc_encrypt
+.globl	_gnutls_padlock_cfb_encrypt
+.type	_gnutls_padlock_cfb_encrypt,@function
 .align	16
-padlock_cfb_encrypt:
+_gnutls_padlock_cfb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -708,11 +708,11 @@ padlock_cfb_encrypt:
 	popq	%rbx
 	popq	%rbp
 	.byte	0xf3,0xc3
-.size	padlock_cfb_encrypt,.-padlock_cfb_encrypt
-.globl	padlock_ofb_encrypt
-.type	padlock_ofb_encrypt,@function
+.size	_gnutls_padlock_cfb_encrypt,.-_gnutls_padlock_cfb_encrypt
+.globl	_gnutls_padlock_ofb_encrypt
+.type	_gnutls_padlock_ofb_encrypt,@function
 .align	16
-padlock_ofb_encrypt:
+_gnutls_padlock_ofb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -822,11 +822,11 @@ padlock_ofb_encrypt:
 	popq	%rbx
 	popq	%rbp
 	.byte	0xf3,0xc3
-.size	padlock_ofb_encrypt,.-padlock_ofb_encrypt
-.globl	padlock_ctr32_encrypt
-.type	padlock_ctr32_encrypt,@function
+.size	_gnutls_padlock_ofb_encrypt,.-_gnutls_padlock_ofb_encrypt
+.globl	_gnutls_padlock_ctr32_encrypt
+.type	_gnutls_padlock_ctr32_encrypt,@function
 .align	16
-padlock_ctr32_encrypt:
+_gnutls_padlock_ctr32_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -1056,7 +1056,7 @@ padlock_ctr32_encrypt:
 	popq	%rbx
 	popq	%rbp
 	.byte	0xf3,0xc3
-.size	padlock_ctr32_encrypt,.-padlock_ctr32_encrypt
+.size	_gnutls_padlock_ctr32_encrypt,.-_gnutls_padlock_ctr32_encrypt
 .byte	86,73,65,32,80,97,100,108,111,99,107,32,120,56,54,95,54,52,32,109,111,100,117,108,101,44,32,67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112,114,111,64,111,112,101,110,115,115,108,46,111,114,103,62,0
 .align	16
 .data	
diff --git a/lib/accelerated/x86/elf/ghash-x86_64.s b/lib/accelerated/x86/elf/ghash-x86_64.s
index 1e4d18b..ad36fde 100644
--- a/lib/accelerated/x86/elf/ghash-x86_64.s
+++ b/lib/accelerated/x86/elf/ghash-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	gcm_gmult_4bit
-.type	gcm_gmult_4bit,@function
+.globl	_gnutls_gcm_gmult_4bit
+.type	_gnutls_gcm_gmult_4bit,@function
 .align	16
-gcm_gmult_4bit:
+_gnutls_gcm_gmult_4bit:
 .cfi_startproc	
 	pushq	%rbx
 .cfi_adjust_cfa_offset	8
@@ -150,11 +150,11 @@ gcm_gmult_4bit:
 .Lgmult_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_gmult_4bit,.-gcm_gmult_4bit
-.globl	gcm_ghash_4bit
-.type	gcm_ghash_4bit,@function
+.size	_gnutls_gcm_gmult_4bit,.-_gnutls_gcm_gmult_4bit
+.globl	_gnutls_gcm_ghash_4bit
+.type	_gnutls_gcm_ghash_4bit,@function
 .align	16
-gcm_ghash_4bit:
+_gnutls_gcm_ghash_4bit:
 .cfi_startproc	
 	pushq	%rbx
 .cfi_adjust_cfa_offset	8
@@ -739,11 +739,11 @@ gcm_ghash_4bit:
 .Lghash_epilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_ghash_4bit,.-gcm_ghash_4bit
-.globl	gcm_init_clmul
-.type	gcm_init_clmul,@function
+.size	_gnutls_gcm_ghash_4bit,.-_gnutls_gcm_ghash_4bit
+.globl	_gnutls_gcm_init_clmul
+.type	_gnutls_gcm_init_clmul,@function
 .align	16
-gcm_init_clmul:
+_gnutls_gcm_init_clmul:
 .cfi_startproc	
 .L_init_clmul:
 	movdqu	(%rsi),%xmm2
@@ -897,11 +897,11 @@ gcm_init_clmul:
 	movdqu	%xmm4,80(%rdi)
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_init_clmul,.-gcm_init_clmul
-.globl	gcm_gmult_clmul
-.type	gcm_gmult_clmul,@function
+.size	_gnutls_gcm_init_clmul,.-_gnutls_gcm_init_clmul
+.globl	_gnutls_gcm_gmult_clmul
+.type	_gnutls_gcm_gmult_clmul,@function
 .align	16
-gcm_gmult_clmul:
+_gnutls_gcm_gmult_clmul:
 .cfi_startproc	
 .L_gmult_clmul:
 	movdqu	(%rdi),%xmm0
@@ -950,11 +950,11 @@ gcm_gmult_clmul:
 	movdqu	%xmm0,(%rdi)
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_gmult_clmul,.-gcm_gmult_clmul
-.globl	gcm_ghash_clmul
-.type	gcm_ghash_clmul,@function
+.size	_gnutls_gcm_gmult_clmul,.-_gnutls_gcm_gmult_clmul
+.globl	_gnutls_gcm_ghash_clmul
+.type	_gnutls_gcm_ghash_clmul,@function
 .align	32
-gcm_ghash_clmul:
+_gnutls_gcm_ghash_clmul:
 .cfi_startproc	
 .L_ghash_clmul:
 	movdqa	.Lbswap_mask(%rip),%xmm10
@@ -1335,11 +1335,11 @@ gcm_ghash_clmul:
 	movdqu	%xmm0,(%rdi)
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_ghash_clmul,.-gcm_ghash_clmul
-.globl	gcm_init_avx
-.type	gcm_init_avx,@function
+.size	_gnutls_gcm_ghash_clmul,.-_gnutls_gcm_ghash_clmul
+.globl	_gnutls_gcm_init_avx
+.type	_gnutls_gcm_init_avx,@function
 .align	32
-gcm_init_avx:
+_gnutls_gcm_init_avx:
 .cfi_startproc	
 	vzeroupper
 
@@ -1444,19 +1444,19 @@ gcm_init_avx:
 	vzeroupper
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_init_avx,.-gcm_init_avx
-.globl	gcm_gmult_avx
-.type	gcm_gmult_avx,@function
+.size	_gnutls_gcm_init_avx,.-_gnutls_gcm_init_avx
+.globl	_gnutls_gcm_gmult_avx
+.type	_gnutls_gcm_gmult_avx,@function
 .align	32
-gcm_gmult_avx:
+_gnutls_gcm_gmult_avx:
 .cfi_startproc	
 	jmp	.L_gmult_clmul
 .cfi_endproc	
-.size	gcm_gmult_avx,.-gcm_gmult_avx
-.globl	gcm_ghash_avx
-.type	gcm_ghash_avx,@function
+.size	_gnutls_gcm_gmult_avx,.-_gnutls_gcm_gmult_avx
+.globl	_gnutls_gcm_ghash_avx
+.type	_gnutls_gcm_ghash_avx,@function
 .align	32
-gcm_ghash_avx:
+_gnutls_gcm_ghash_avx:
 .cfi_startproc	
 	vzeroupper
 
@@ -1830,7 +1830,7 @@ gcm_ghash_avx:
 	vzeroupper
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	gcm_ghash_avx,.-gcm_ghash_avx
+.size	_gnutls_gcm_ghash_avx,.-_gnutls_gcm_ghash_avx
 .align	64
 .Lbswap_mask:
 .byte	15,14,13,12,11,10,9,8,7,6,5,4,3,2,1,0
diff --git a/lib/accelerated/x86/elf/sha1-ssse3-x86.s b/lib/accelerated/x86/elf/sha1-ssse3-x86.s
index 8bfbcb6..692da56 100644
--- a/lib/accelerated/x86/elf/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha1-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	sha1_block_data_order
-.type	sha1_block_data_order,@function
+.globl	_gnutls_sha1_block_data_order
+.type	_gnutls_sha1_block_data_order,@function
 .align	16
-sha1_block_data_order:
+_gnutls_sha1_block_data_order:
 .L_sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1411,7 +1411,7 @@ sha1_block_data_order:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	sha1_block_data_order,.-.L_sha1_block_data_order_begin
+.size	_gnutls_sha1_block_data_order,.-.L_sha1_block_data_order_begin
 .byte	83,72,65,49,32,98,108,111,99,107,32,116,114,97,110,115
 .byte	102,111,114,109,32,102,111,114,32,120,56,54,44,32,67,82
 .byte	89,80,84,79,71,65,77,83,32,98,121,32,60,97,112,112
diff --git a/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s b/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
index 1e6546e..a3ecb06 100644
--- a/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha1_block_data_order
-.type	sha1_block_data_order,@function
+.globl	_gnutls_sha1_block_data_order
+.type	_gnutls_sha1_block_data_order,@function
 .align	16
-sha1_block_data_order:
+_gnutls_sha1_block_data_order:
 .cfi_startproc	
 	movl	_gnutls_x86_cpuid_s+0(%rip),%r9d
 	movl	_gnutls_x86_cpuid_s+4(%rip),%r8d
@@ -1293,7 +1293,7 @@ sha1_block_data_order:
 .Lepilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	sha1_block_data_order,.-sha1_block_data_order
+.size	_gnutls_sha1_block_data_order,.-_gnutls_sha1_block_data_order
 .type	sha1_block_data_order_shaext,@function
 .align	32
 sha1_block_data_order_shaext:
diff --git a/lib/accelerated/x86/elf/sha256-ssse3-x86.s b/lib/accelerated/x86/elf/sha256-ssse3-x86.s
index 8d9aaa4..49d6b35 100644
--- a/lib/accelerated/x86/elf/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha256-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	sha256_block_data_order
-.type	sha256_block_data_order,@function
+.globl	_gnutls_sha256_block_data_order
+.type	_gnutls_sha256_block_data_order,@function
 .align	16
-sha256_block_data_order:
+_gnutls_sha256_block_data_order:
 .L_sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -3382,6 +3382,6 @@ sha256_block_data_order:
 	popl	%ebx
 	popl	%ebp
 	ret
-.size	sha256_block_data_order,.-.L_sha256_block_data_order_begin
+.size	_gnutls_sha256_block_data_order,.-.L_sha256_block_data_order_begin
 
 .section .note.GNU-stack,"",%progbits
diff --git a/lib/accelerated/x86/elf/sha256-ssse3-x86_64.s b/lib/accelerated/x86/elf/sha256-ssse3-x86_64.s
index 4b08e0c..44cf685 100644
--- a/lib/accelerated/x86/elf/sha256-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/sha256-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha256_block_data_order
-.type	sha256_block_data_order,@function
+.globl	_gnutls_sha256_block_data_order
+.type	_gnutls_sha256_block_data_order,@function
 .align	16
-sha256_block_data_order:
+_gnutls_sha256_block_data_order:
 .cfi_startproc	
 	leaq	_gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
@@ -1766,7 +1766,7 @@ sha256_block_data_order:
 .Lepilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	sha256_block_data_order,.-sha256_block_data_order
+.size	_gnutls_sha256_block_data_order,.-_gnutls_sha256_block_data_order
 .align	64
 .type	K256,@object
 K256:
diff --git a/lib/accelerated/x86/elf/sha512-ssse3-x86.s b/lib/accelerated/x86/elf/sha512-ssse3-x86.s
index 481c777..052e169 100644
--- a/lib/accelerated/x86/elf/sha512-ssse3-x86.s
+++ b/lib/accelerated/x86/elf/sha512-ssse3-x86.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	sha512_block_data_order
-.type	sha512_block_data_order,@function
+.globl	_gnutls_sha512_block_data_order
+.type	_gnutls_sha512_block_data_order,@function
 .align	16
-sha512_block_data_order:
+_gnutls_sha512_block_data_order:
 .L_sha512_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -595,7 +595,7 @@ sha512_block_data_order:
 .long	1246189591,1816402316
 .long	67438087,66051
 .long	202182159,134810123
-.size	sha512_block_data_order,.-.L_sha512_block_data_order_begin
+.size	_gnutls_sha512_block_data_order,.-.L_sha512_block_data_order_begin
 .byte	83,72,65,53,49,50,32,98,108,111,99,107,32,116,114,97
 .byte	110,115,102,111,114,109,32,102,111,114,32,120,56,54,44,32
 .byte	67,82,89,80,84,79,71,65,77,83,32,98,121,32,60,97
diff --git a/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s b/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
index e384d7e..1c9185d 100644
--- a/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/elf/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	sha512_block_data_order
-.type	sha512_block_data_order,@function
+.globl	_gnutls_sha512_block_data_order
+.type	_gnutls_sha512_block_data_order,@function
 .align	16
-sha512_block_data_order:
+_gnutls_sha512_block_data_order:
 .cfi_startproc	
 	leaq	_gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
@@ -1764,7 +1764,7 @@ sha512_block_data_order:
 .Lepilogue:
 	.byte	0xf3,0xc3
 .cfi_endproc	
-.size	sha512_block_data_order,.-sha512_block_data_order
+.size	_gnutls_sha512_block_data_order,.-_gnutls_sha512_block_data_order
 .align	64
 .type	K512,@object
 K512:
diff --git a/lib/accelerated/x86/macosx/aes-ssse3-x86.s b/lib/accelerated/x86/macosx/aes-ssse3-x86.s
index 4be8992..e0adc61 100644
--- a/lib/accelerated/x86/macosx/aes-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/aes-ssse3-x86.s
@@ -454,9 +454,9 @@ L015schedule_mangle_both:
 	andl	$48,%ecx
 	movdqu	%xmm3,(%edx)
 	ret
-.globl	_vpaes_set_encrypt_key
+.globl	__gnutls_vpaes_set_encrypt_key
 .align	4
-_vpaes_set_encrypt_key:
+__gnutls_vpaes_set_encrypt_key:
 L_vpaes_set_encrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -485,9 +485,9 @@ L016pic_point:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_set_decrypt_key
+.globl	__gnutls_vpaes_set_decrypt_key
 .align	4
-_vpaes_set_decrypt_key:
+__gnutls_vpaes_set_decrypt_key:
 L_vpaes_set_decrypt_key_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -521,9 +521,9 @@ L017pic_point:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_encrypt
+.globl	__gnutls_vpaes_encrypt
 .align	4
-_vpaes_encrypt:
+__gnutls_vpaes_encrypt:
 L_vpaes_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -548,9 +548,9 @@ L018pic_point:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_decrypt
+.globl	__gnutls_vpaes_decrypt
 .align	4
-_vpaes_decrypt:
+__gnutls_vpaes_decrypt:
 L_vpaes_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -575,9 +575,9 @@ L019pic_point:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_vpaes_cbc_encrypt
+.globl	__gnutls_vpaes_cbc_encrypt
 .align	4
-_vpaes_cbc_encrypt:
+__gnutls_vpaes_cbc_encrypt:
 L_vpaes_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/macosx/aes-ssse3-x86_64.s b/lib/accelerated/x86/macosx/aes-ssse3-x86_64.s
index 3d5c652..a78c3e9 100644
--- a/lib/accelerated/x86/macosx/aes-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/aes-ssse3-x86_64.s
@@ -630,10 +630,10 @@ L$schedule_mangle_both:
 
 
 
-.globl	_vpaes_set_encrypt_key
+.globl	__gnutls_vpaes_set_encrypt_key
 
 .p2align	4
-_vpaes_set_encrypt_key:
+__gnutls_vpaes_set_encrypt_key:
 
 	movl	%esi,%eax
 	shrl	$5,%eax
@@ -648,10 +648,10 @@ _vpaes_set_encrypt_key:
 
 
 
-.globl	_vpaes_set_decrypt_key
+.globl	__gnutls_vpaes_set_decrypt_key
 
 .p2align	4
-_vpaes_set_decrypt_key:
+__gnutls_vpaes_set_decrypt_key:
 
 	movl	%esi,%eax
 	shrl	$5,%eax
@@ -671,10 +671,10 @@ _vpaes_set_decrypt_key:
 
 
 
-.globl	_vpaes_encrypt
+.globl	__gnutls_vpaes_encrypt
 
 .p2align	4
-_vpaes_encrypt:
+__gnutls_vpaes_encrypt:
 
 	movdqu	(%rdi),%xmm0
 	call	_vpaes_preheat
@@ -684,10 +684,10 @@ _vpaes_encrypt:
 
 
 
-.globl	_vpaes_decrypt
+.globl	__gnutls_vpaes_decrypt
 
 .p2align	4
-_vpaes_decrypt:
+__gnutls_vpaes_decrypt:
 
 	movdqu	(%rdi),%xmm0
 	call	_vpaes_preheat
@@ -696,10 +696,10 @@ _vpaes_decrypt:
 	.byte	0xf3,0xc3
 
 
-.globl	_vpaes_cbc_encrypt
+.globl	__gnutls_vpaes_cbc_encrypt
 
 .p2align	4
-_vpaes_cbc_encrypt:
+__gnutls_vpaes_cbc_encrypt:
 
 	xchgq	%rcx,%rdx
 	subq	$16,%rcx
diff --git a/lib/accelerated/x86/macosx/aesni-gcm-x86_64.s b/lib/accelerated/x86/macosx/aesni-gcm-x86_64.s
index d540930..e479e8f 100644
--- a/lib/accelerated/x86/macosx/aesni-gcm-x86_64.s
+++ b/lib/accelerated/x86/macosx/aesni-gcm-x86_64.s
@@ -350,10 +350,10 @@ L$6x_done:
 
 	.byte	0xf3,0xc3
 
-.globl	_aesni_gcm_decrypt
+.globl	__gnutls_aesni_gcm_decrypt
 
 .p2align	5
-_aesni_gcm_decrypt:
+__gnutls_aesni_gcm_decrypt:
 
 	xorq	%r10,%r10
 	cmpq	$0x60,%rdx
@@ -543,10 +543,10 @@ L$handle_ctr32_2:
 	jmp	L$oop_ctr32
 
 
-.globl	_aesni_gcm_encrypt
+.globl	__gnutls_aesni_gcm_encrypt
 
 .p2align	5
-_aesni_gcm_encrypt:
+__gnutls_aesni_gcm_encrypt:
 
 	xorq	%r10,%r10
 	cmpq	$288,%rdx
diff --git a/lib/accelerated/x86/macosx/aesni-x86.s b/lib/accelerated/x86/macosx/aesni-x86.s
index ee50089..a7048fe 100644
--- a/lib/accelerated/x86/macosx/aesni-x86.s
+++ b/lib/accelerated/x86/macosx/aesni-x86.s
@@ -38,9 +38,9 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_aesni_encrypt
+.globl	__gnutls_aesni_encrypt
 .align	4
-_aesni_encrypt:
+__gnutls_aesni_encrypt:
 L_aesni_encrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -63,9 +63,9 @@ L000enc1_loop_1:
 	movups	%xmm2,(%eax)
 	pxor	%xmm2,%xmm2
 	ret
-.globl	_aesni_decrypt
+.globl	__gnutls_aesni_decrypt
 .align	4
-_aesni_decrypt:
+__gnutls_aesni_decrypt:
 L_aesni_decrypt_begin:
 	movl	4(%esp),%eax
 	movl	12(%esp),%edx
@@ -374,9 +374,9 @@ L_aesni_decrypt6_enter:
 .byte	102,15,56,223,240
 .byte	102,15,56,223,248
 	ret
-.globl	_aesni_ecb_encrypt
+.globl	__gnutls_aesni_ecb_encrypt
 .align	4
-_aesni_ecb_encrypt:
+__gnutls_aesni_ecb_encrypt:
 L_aesni_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -607,9 +607,9 @@ L012ecb_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_encrypt_blocks
+.globl	__gnutls_aesni_ccm64_encrypt_blocks
 .align	4
-_aesni_ccm64_encrypt_blocks:
+__gnutls_aesni_ccm64_encrypt_blocks:
 L_aesni_ccm64_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -693,9 +693,9 @@ L031ccm64_enc2_loop:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ccm64_decrypt_blocks
+.globl	__gnutls_aesni_ccm64_decrypt_blocks
 .align	4
-_aesni_ccm64_decrypt_blocks:
+__gnutls_aesni_ccm64_decrypt_blocks:
 L_aesni_ccm64_decrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -814,9 +814,9 @@ L036enc1_loop_6:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ctr32_encrypt_blocks
+.globl	__gnutls_aesni_ctr32_encrypt_blocks
 .align	4
-_aesni_ctr32_encrypt_blocks:
+__gnutls_aesni_ctr32_encrypt_blocks:
 L_aesni_ctr32_encrypt_blocks_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1050,9 +1050,9 @@ L040ctr32_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_encrypt
+.globl	__gnutls_aesni_xts_encrypt
 .align	4
-_aesni_xts_encrypt:
+__gnutls_aesni_xts_encrypt:
 L_aesni_xts_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1408,9 +1408,9 @@ L056xts_enc_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_xts_decrypt
+.globl	__gnutls_aesni_xts_decrypt
 .align	4
-_aesni_xts_decrypt:
+__gnutls_aesni_xts_decrypt:
 L_aesni_xts_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -1796,9 +1796,9 @@ L069xts_dec_ret:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ocb_encrypt
+.globl	__gnutls_aesni_ocb_encrypt
 .align	4
-_aesni_ocb_encrypt:
+__gnutls_aesni_ocb_encrypt:
 L_aesni_ocb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2189,9 +2189,9 @@ L078done:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_ocb_decrypt
+.globl	__gnutls_aesni_ocb_decrypt
 .align	4
-_aesni_ocb_decrypt:
+__gnutls_aesni_ocb_decrypt:
 L_aesni_ocb_decrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -2582,9 +2582,9 @@ L088done:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_cbc_encrypt
+.globl	__gnutls_aesni_cbc_encrypt
 .align	4
-_aesni_cbc_encrypt:
+__gnutls_aesni_cbc_encrypt:
 L_aesni_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -3172,18 +3172,18 @@ L115bad_keybits:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_aesni_set_encrypt_key
+.globl	__gnutls_aesni_set_encrypt_key
 .align	4
-_aesni_set_encrypt_key:
+__gnutls_aesni_set_encrypt_key:
 L_aesni_set_encrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
 	movl	12(%esp),%edx
 	call	__aesni_set_encrypt_key
 	ret
-.globl	_aesni_set_decrypt_key
+.globl	__gnutls_aesni_set_decrypt_key
 .align	4
-_aesni_set_decrypt_key:
+__gnutls_aesni_set_decrypt_key:
 L_aesni_set_decrypt_key_begin:
 	movl	4(%esp),%eax
 	movl	8(%esp),%ecx
diff --git a/lib/accelerated/x86/macosx/aesni-x86_64.s b/lib/accelerated/x86/macosx/aesni-x86_64.s
index f6145f1..8341788 100644
--- a/lib/accelerated/x86/macosx/aesni-x86_64.s
+++ b/lib/accelerated/x86/macosx/aesni-x86_64.s
@@ -39,10 +39,10 @@
 #
 .text	
 
-.globl	_aesni_encrypt
+.globl	__gnutls_aesni_encrypt
 
 .p2align	4
-_aesni_encrypt:
+__gnutls_aesni_encrypt:
 
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
@@ -65,10 +65,10 @@ L$oop_enc1_1:
 
 
 
-.globl	_aesni_decrypt
+.globl	__gnutls_aesni_decrypt
 
 .p2align	4
-_aesni_decrypt:
+__gnutls_aesni_decrypt:
 
 	movups	(%rdi),%xmm2
 	movl	240(%rdx),%eax
@@ -552,10 +552,10 @@ L$dec_loop8_enter:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_ecb_encrypt
+.globl	__gnutls_aesni_ecb_encrypt
 
 .p2align	4
-_aesni_ecb_encrypt:
+__gnutls_aesni_ecb_encrypt:
 
 	andq	$-16,%rdx
 	jz	L$ecb_ret
@@ -896,10 +896,10 @@ L$ecb_ret:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_ccm64_encrypt_blocks
+.globl	__gnutls_aesni_ccm64_encrypt_blocks
 
 .p2align	4
-_aesni_ccm64_encrypt_blocks:
+__gnutls_aesni_ccm64_encrypt_blocks:
 	movl	240(%rcx),%eax
 	movdqu	(%r8),%xmm6
 	movdqa	L$increment64(%rip),%xmm9
@@ -959,10 +959,10 @@ L$ccm64_enc2_loop:
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ccm64_decrypt_blocks
+.globl	__gnutls_aesni_ccm64_decrypt_blocks
 
 .p2align	4
-_aesni_ccm64_decrypt_blocks:
+__gnutls_aesni_ccm64_decrypt_blocks:
 	movl	240(%rcx),%eax
 	movups	(%r8),%xmm6
 	movdqu	(%r9),%xmm3
@@ -1056,10 +1056,10 @@ L$oop_enc1_6:
 	pxor	%xmm6,%xmm6
 	.byte	0xf3,0xc3
 
-.globl	_aesni_ctr32_encrypt_blocks
+.globl	__gnutls_aesni_ctr32_encrypt_blocks
 
 .p2align	4
-_aesni_ctr32_encrypt_blocks:
+__gnutls_aesni_ctr32_encrypt_blocks:
 
 	cmpq	$1,%rdx
 	jne	L$ctr32_bulk
@@ -1634,10 +1634,10 @@ L$ctr32_epilogue:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_xts_encrypt
+.globl	__gnutls_aesni_xts_encrypt
 
 .p2align	4
-_aesni_xts_encrypt:
+__gnutls_aesni_xts_encrypt:
 
 	leaq	(%rsp),%r11
 
@@ -2104,10 +2104,10 @@ L$xts_enc_epilogue:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_xts_decrypt
+.globl	__gnutls_aesni_xts_decrypt
 
 .p2align	4
-_aesni_xts_decrypt:
+__gnutls_aesni_xts_decrypt:
 
 	leaq	(%rsp),%r11
 
@@ -2611,10 +2611,10 @@ L$xts_dec_epilogue:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_ocb_encrypt
+.globl	__gnutls_aesni_ocb_encrypt
 
 .p2align	5
-_aesni_ocb_encrypt:
+__gnutls_aesni_ocb_encrypt:
 
 	leaq	(%rsp),%rax
 	pushq	%rbx
@@ -3027,10 +3027,10 @@ L$ocb_enc_loop1:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_ocb_decrypt
+.globl	__gnutls_aesni_ocb_decrypt
 
 .p2align	5
-_aesni_ocb_decrypt:
+__gnutls_aesni_ocb_decrypt:
 
 	leaq	(%rsp),%rax
 	pushq	%rbx
@@ -3453,10 +3453,10 @@ L$ocb_dec_loop1:
 .byte	102,15,56,223,215
 	.byte	0xf3,0xc3
 
-.globl	_aesni_cbc_encrypt
+.globl	__gnutls_aesni_cbc_encrypt
 
 .p2align	4
-_aesni_cbc_encrypt:
+__gnutls_aesni_cbc_encrypt:
 
 	testq	%rdx,%rdx
 	jz	L$cbc_ret
@@ -4044,10 +4044,10 @@ L$cbc_ret:
 	.byte	0xf3,0xc3
 
 
-.globl	_aesni_set_decrypt_key
+.globl	__gnutls_aesni_set_decrypt_key
 
 .p2align	4
-_aesni_set_decrypt_key:
+__gnutls_aesni_set_decrypt_key:
 
 .byte	0x48,0x83,0xEC,0x08
 
@@ -4088,10 +4088,10 @@ L$dec_key_ret:
 
 L$SEH_end_set_decrypt_key:
 
-.globl	_aesni_set_encrypt_key
+.globl	__gnutls_aesni_set_encrypt_key
 
 .p2align	4
-_aesni_set_encrypt_key:
+__gnutls_aesni_set_encrypt_key:
 __aesni_set_encrypt_key:
 
 .byte	0x48,0x83,0xEC,0x08
diff --git a/lib/accelerated/x86/macosx/e_padlock-x86.s b/lib/accelerated/x86/macosx/e_padlock-x86.s
index 367962c..7559632 100644
--- a/lib/accelerated/x86/macosx/e_padlock-x86.s
+++ b/lib/accelerated/x86/macosx/e_padlock-x86.s
@@ -39,9 +39,9 @@
 #
 .file	"devel/perlasm/e_padlock-x86.s"
 .text
-.globl	_padlock_capability
+.globl	__gnutls_padlock_capability
 .align	4
-_padlock_capability:
+__gnutls_padlock_capability:
 L_padlock_capability_begin:
 	pushl	%ebx
 	pushfl
@@ -88,9 +88,9 @@ L_padlock_capability_begin:
 L000noluck:
 	popl	%ebx
 	ret
-.globl	_padlock_key_bswap
+.globl	__gnutls_padlock_key_bswap
 .align	4
-_padlock_key_bswap:
+__gnutls_padlock_key_bswap:
 L_padlock_key_bswap_begin:
 	movl	4(%esp),%edx
 	movl	240(%edx),%ecx
@@ -102,9 +102,9 @@ L001bswap_loop:
 	subl	$1,%ecx
 	jnz	L001bswap_loop
 	ret
-.globl	_padlock_verify_context
+.globl	__gnutls_padlock_verify_context
 .align	4
-_padlock_verify_context:
+__gnutls_padlock_verify_context:
 L_padlock_verify_context_begin:
 	movl	4(%esp),%edx
 	leal	Lpadlock_saved_context-L002verify_pic_point,%eax
@@ -125,16 +125,16 @@ __padlock_verify_ctx:
 L003verified:
 	movl	%edx,(%eax)
 	ret
-.globl	_padlock_reload_key
+.globl	__gnutls_padlock_reload_key
 .align	4
-_padlock_reload_key:
+__gnutls_padlock_reload_key:
 L_padlock_reload_key_begin:
 	pushfl
 	popfl
 	ret
-.globl	_padlock_aes_block
+.globl	__gnutls_padlock_aes_block
 .align	4
-_padlock_aes_block:
+__gnutls_padlock_aes_block:
 L_padlock_aes_block_begin:
 	pushl	%edi
 	pushl	%esi
@@ -150,9 +150,9 @@ L_padlock_aes_block_begin:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_ecb_encrypt
+.globl	__gnutls_padlock_ecb_encrypt
 .align	4
-_padlock_ecb_encrypt:
+__gnutls_padlock_ecb_encrypt:
 L_padlock_ecb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -327,9 +327,9 @@ L004ecb_abort:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_cbc_encrypt
+.globl	__gnutls_padlock_cbc_encrypt
 .align	4
-_padlock_cbc_encrypt:
+__gnutls_padlock_cbc_encrypt:
 L_padlock_cbc_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -508,9 +508,9 @@ L016cbc_abort:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_cfb_encrypt
+.globl	__gnutls_padlock_cfb_encrypt
 .align	4
-_padlock_cfb_encrypt:
+__gnutls_padlock_cfb_encrypt:
 L_padlock_cfb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -628,9 +628,9 @@ L028cfb_abort:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_ofb_encrypt
+.globl	__gnutls_padlock_ofb_encrypt
 .align	4
-_padlock_ofb_encrypt:
+__gnutls_padlock_ofb_encrypt:
 L_padlock_ofb_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -748,9 +748,9 @@ L037ofb_abort:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_ctr32_encrypt
+.globl	__gnutls_padlock_ctr32_encrypt
 .align	4
-_padlock_ctr32_encrypt:
+__gnutls_padlock_ctr32_encrypt:
 L_padlock_ctr32_encrypt_begin:
 	pushl	%ebp
 	pushl	%ebx
@@ -853,9 +853,9 @@ L046ctr32_abort:
 	popl	%ebx
 	popl	%ebp
 	ret
-.globl	_padlock_xstore
+.globl	__gnutls_padlock_xstore
 .align	4
-_padlock_xstore:
+__gnutls_padlock_xstore:
 L_padlock_xstore_begin:
 	pushl	%edi
 	movl	8(%esp),%edi
@@ -874,9 +874,9 @@ __win32_segv_handler:
 	movl	$0,%eax
 L053ret:
 	ret
-.globl	_padlock_sha1_oneshot
+.globl	__gnutls_padlock_sha1_oneshot
 .align	4
-_padlock_sha1_oneshot:
+__gnutls_padlock_sha1_oneshot:
 L_padlock_sha1_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -903,9 +903,9 @@ L_padlock_sha1_oneshot_begin:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha1_blocks
+.globl	__gnutls_padlock_sha1_blocks
 .align	4
-_padlock_sha1_blocks:
+__gnutls_padlock_sha1_blocks:
 L_padlock_sha1_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -931,9 +931,9 @@ L_padlock_sha1_blocks_begin:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha256_oneshot
+.globl	__gnutls_padlock_sha256_oneshot
 .align	4
-_padlock_sha256_oneshot:
+__gnutls_padlock_sha256_oneshot:
 L_padlock_sha256_oneshot_begin:
 	pushl	%edi
 	pushl	%esi
@@ -960,9 +960,9 @@ L_padlock_sha256_oneshot_begin:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha256_blocks
+.globl	__gnutls_padlock_sha256_blocks
 .align	4
-_padlock_sha256_blocks:
+__gnutls_padlock_sha256_blocks:
 L_padlock_sha256_blocks_begin:
 	pushl	%edi
 	pushl	%esi
@@ -988,9 +988,9 @@ L_padlock_sha256_blocks_begin:
 	popl	%esi
 	popl	%edi
 	ret
-.globl	_padlock_sha512_blocks
+.globl	__gnutls_padlock_sha512_blocks
 .align	4
-_padlock_sha512_blocks:
+__gnutls_padlock_sha512_blocks:
 L_padlock_sha512_blocks_begin:
 	pushl	%edi
 	pushl	%esi
diff --git a/lib/accelerated/x86/macosx/e_padlock-x86_64.s b/lib/accelerated/x86/macosx/e_padlock-x86_64.s
index a73d7a6..d367a6a 100644
--- a/lib/accelerated/x86/macosx/e_padlock-x86_64.s
+++ b/lib/accelerated/x86/macosx/e_padlock-x86_64.s
@@ -38,10 +38,10 @@
 # *** This file is auto-generated ***
 #
 .text	
-.globl	_padlock_capability
+.globl	__gnutls_padlock_capability
 
 .p2align	4
-_padlock_capability:
+__gnutls_padlock_capability:
 	movq	%rbx,%r8
 	xorl	%eax,%eax
 	cpuid
@@ -68,10 +68,10 @@ L$noluck:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_key_bswap
+.globl	__gnutls_padlock_key_bswap
 
 .p2align	4
-_padlock_key_bswap:
+__gnutls_padlock_key_bswap:
 	movl	240(%rdi),%edx
 L$bswap_loop:
 	movl	(%rdi),%eax
@@ -83,10 +83,10 @@ L$bswap_loop:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_verify_context
+.globl	__gnutls_padlock_verify_context
 
 .p2align	4
-_padlock_verify_context:
+__gnutls_padlock_verify_context:
 	movq	%rdi,%rdx
 	pushf
 	leaq	L$padlock_saved_context(%rip),%rax
@@ -110,19 +110,19 @@ L$verified:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_reload_key
+.globl	__gnutls_padlock_reload_key
 
 .p2align	4
-_padlock_reload_key:
+__gnutls_padlock_reload_key:
 	pushf
 	popf
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_aes_block
+.globl	__gnutls_padlock_aes_block
 
 .p2align	4
-_padlock_aes_block:
+__gnutls_padlock_aes_block:
 	movq	%rbx,%r8
 	movq	$1,%rcx
 	leaq	32(%rdx),%rbx
@@ -132,19 +132,19 @@ _padlock_aes_block:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_xstore
+.globl	__gnutls_padlock_xstore
 
 .p2align	4
-_padlock_xstore:
+__gnutls_padlock_xstore:
 	movl	%esi,%edx
 .byte	0x0f,0xa7,0xc0		
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_sha1_oneshot
+.globl	__gnutls_padlock_sha1_oneshot
 
 .p2align	4
-_padlock_sha1_oneshot:
+__gnutls_padlock_sha1_oneshot:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -163,10 +163,10 @@ _padlock_sha1_oneshot:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_sha1_blocks
+.globl	__gnutls_padlock_sha1_blocks
 
 .p2align	4
-_padlock_sha1_blocks:
+__gnutls_padlock_sha1_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -185,10 +185,10 @@ _padlock_sha1_blocks:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_sha256_oneshot
+.globl	__gnutls_padlock_sha256_oneshot
 
 .p2align	4
-_padlock_sha256_oneshot:
+__gnutls_padlock_sha256_oneshot:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -207,10 +207,10 @@ _padlock_sha256_oneshot:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_sha256_blocks
+.globl	__gnutls_padlock_sha256_blocks
 
 .p2align	4
-_padlock_sha256_blocks:
+__gnutls_padlock_sha256_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -229,10 +229,10 @@ _padlock_sha256_blocks:
 	.byte	0xf3,0xc3
 
 
-.globl	_padlock_sha512_blocks
+.globl	__gnutls_padlock_sha512_blocks
 
 .p2align	4
-_padlock_sha512_blocks:
+__gnutls_padlock_sha512_blocks:
 	movq	%rdx,%rcx
 	movq	%rdi,%rdx
 	movups	(%rdi),%xmm0
@@ -257,10 +257,10 @@ _padlock_sha512_blocks:
 	movups	%xmm3,48(%rdx)
 	.byte	0xf3,0xc3
 
-.globl	_padlock_ecb_encrypt
+.globl	__gnutls_padlock_ecb_encrypt
 
 .p2align	4
-_padlock_ecb_encrypt:
+__gnutls_padlock_ecb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -424,10 +424,10 @@ L$ecb_abort:
 	popq	%rbp
 	.byte	0xf3,0xc3
 
-.globl	_padlock_cbc_encrypt
+.globl	__gnutls_padlock_cbc_encrypt
 
 .p2align	4
-_padlock_cbc_encrypt:
+__gnutls_padlock_cbc_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -595,10 +595,10 @@ L$cbc_abort:
 	popq	%rbp
 	.byte	0xf3,0xc3
 
-.globl	_padlock_cfb_encrypt
+.globl	__gnutls_padlock_cfb_encrypt
 
 .p2align	4
-_padlock_cfb_encrypt:
+__gnutls_padlock_cfb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -709,10 +709,10 @@ L$cfb_abort:
 	popq	%rbp
 	.byte	0xf3,0xc3
 
-.globl	_padlock_ofb_encrypt
+.globl	__gnutls_padlock_ofb_encrypt
 
 .p2align	4
-_padlock_ofb_encrypt:
+__gnutls_padlock_ofb_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
@@ -823,10 +823,10 @@ L$ofb_abort:
 	popq	%rbp
 	.byte	0xf3,0xc3
 
-.globl	_padlock_ctr32_encrypt
+.globl	__gnutls_padlock_ctr32_encrypt
 
 .p2align	4
-_padlock_ctr32_encrypt:
+__gnutls_padlock_ctr32_encrypt:
 	pushq	%rbp
 	pushq	%rbx
 
diff --git a/lib/accelerated/x86/macosx/ghash-x86_64.s b/lib/accelerated/x86/macosx/ghash-x86_64.s
index 5fd3216..65ce83d 100644
--- a/lib/accelerated/x86/macosx/ghash-x86_64.s
+++ b/lib/accelerated/x86/macosx/ghash-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_gcm_gmult_4bit
+.globl	__gnutls_gcm_gmult_4bit
 
 .p2align	4
-_gcm_gmult_4bit:
+__gnutls_gcm_gmult_4bit:
 
 	pushq	%rbx
 
@@ -145,10 +145,10 @@ L$gmult_epilogue:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_ghash_4bit
+.globl	__gnutls_gcm_ghash_4bit
 
 .p2align	4
-_gcm_ghash_4bit:
+__gnutls_gcm_ghash_4bit:
 
 	pushq	%rbx
 
@@ -728,10 +728,10 @@ L$ghash_epilogue:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_init_clmul
+.globl	__gnutls_gcm_init_clmul
 
 .p2align	4
-_gcm_init_clmul:
+__gnutls_gcm_init_clmul:
 
 L$_init_clmul:
 	movdqu	(%rsi),%xmm2
@@ -886,10 +886,10 @@ L$_init_clmul:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_gmult_clmul
+.globl	__gnutls_gcm_gmult_clmul
 
 .p2align	4
-_gcm_gmult_clmul:
+__gnutls_gcm_gmult_clmul:
 
 L$_gmult_clmul:
 	movdqu	(%rdi),%xmm0
@@ -939,10 +939,10 @@ L$_gmult_clmul:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_ghash_clmul
+.globl	__gnutls_gcm_ghash_clmul
 
 .p2align	5
-_gcm_ghash_clmul:
+__gnutls_gcm_ghash_clmul:
 
 L$_ghash_clmul:
 	movdqa	L$bswap_mask(%rip),%xmm10
@@ -1324,10 +1324,10 @@ L$done:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_init_avx
+.globl	__gnutls_gcm_init_avx
 
 .p2align	5
-_gcm_init_avx:
+__gnutls_gcm_init_avx:
 
 	vzeroupper
 
@@ -1433,18 +1433,18 @@ L$init_start_avx:
 	.byte	0xf3,0xc3
 
 
-.globl	_gcm_gmult_avx
+.globl	__gnutls_gcm_gmult_avx
 
 .p2align	5
-_gcm_gmult_avx:
+__gnutls_gcm_gmult_avx:
 
 	jmp	L$_gmult_clmul
 
 
-.globl	_gcm_ghash_avx
+.globl	__gnutls_gcm_ghash_avx
 
 .p2align	5
-_gcm_ghash_avx:
+__gnutls_gcm_ghash_avx:
 
 	vzeroupper
 
diff --git a/lib/accelerated/x86/macosx/sha1-ssse3-x86.s b/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
index 985d4af..ed20e5a 100644
--- a/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/sha1-ssse3-x86.s
@@ -38,9 +38,9 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha1_block_data_order
+.globl	__gnutls_sha1_block_data_order
 .align	4
-_sha1_block_data_order:
+__gnutls_sha1_block_data_order:
 L_sha1_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s b/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
index a576acc..9503f19 100644
--- a/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/sha1-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_sha1_block_data_order
+.globl	__gnutls_sha1_block_data_order
 
 .p2align	4
-_sha1_block_data_order:
+__gnutls_sha1_block_data_order:
 
 	movl	__gnutls_x86_cpuid_s+0(%rip),%r9d
 	movl	__gnutls_x86_cpuid_s+4(%rip),%r8d
diff --git a/lib/accelerated/x86/macosx/sha256-ssse3-x86.s b/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
index 8d25710..c42d6bf 100644
--- a/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/sha256-ssse3-x86.s
@@ -38,9 +38,9 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha256_block_data_order
+.globl	__gnutls_sha256_block_data_order
 .align	4
-_sha256_block_data_order:
+__gnutls_sha256_block_data_order:
 L_sha256_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/macosx/sha256-ssse3-x86_64.s b/lib/accelerated/x86/macosx/sha256-ssse3-x86_64.s
index fd0c247..34f354f 100644
--- a/lib/accelerated/x86/macosx/sha256-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/sha256-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_sha256_block_data_order
+.globl	__gnutls_sha256_block_data_order
 
 .p2align	4
-_sha256_block_data_order:
+__gnutls_sha256_block_data_order:
 
 	leaq	__gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
diff --git a/lib/accelerated/x86/macosx/sha512-ssse3-x86.s b/lib/accelerated/x86/macosx/sha512-ssse3-x86.s
index 4e60bb4..b9e486c 100644
--- a/lib/accelerated/x86/macosx/sha512-ssse3-x86.s
+++ b/lib/accelerated/x86/macosx/sha512-ssse3-x86.s
@@ -38,9 +38,9 @@
 # *** This file is auto-generated ***
 #
 .text
-.globl	_sha512_block_data_order
+.globl	__gnutls_sha512_block_data_order
 .align	4
-_sha512_block_data_order:
+__gnutls_sha512_block_data_order:
 L_sha512_block_data_order_begin:
 	pushl	%ebp
 	pushl	%ebx
diff --git a/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s b/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
index 8bf1616..bc1e6aa 100644
--- a/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
+++ b/lib/accelerated/x86/macosx/sha512-ssse3-x86_64.s
@@ -40,10 +40,10 @@
 .text	
 
 
-.globl	_sha512_block_data_order
+.globl	__gnutls_sha512_block_data_order
 
 .p2align	4
-_sha512_block_data_order:
+__gnutls_sha512_block_data_order:
 
 	leaq	__gnutls_x86_cpuid_s(%rip),%r11
 	movl	0(%r11),%r9d
diff --git a/lib/accelerated/x86/sha-padlock.c b/lib/accelerated/x86/sha-padlock.c
index e878ebe..4478a9b 100644
--- a/lib/accelerated/x86/sha-padlock.c
+++ b/lib/accelerated/x86/sha-padlock.c
@@ -71,9 +71,9 @@ static void wrap_padlock_hash_deinit(void *hd)
 }
 
 #define MD1_INCR(c) (c->count++)
-#define SHA1_COMPRESS(ctx, data) (padlock_sha1_blocks((void*)(ctx)->state, data, 1))
-#define SHA256_COMPRESS(ctx, data) (padlock_sha256_blocks((void*)(ctx)->state, data, 1))
-#define SHA512_COMPRESS(ctx, data) (padlock_sha512_blocks((void*)(ctx)->state, data, 1))
+#define SHA1_COMPRESS(ctx, data) (_gnutls_padlock_sha1_blocks((void*)(ctx)->state, data, 1))
+#define SHA256_COMPRESS(ctx, data) (_gnutls_padlock_sha256_blocks((void*)(ctx)->state, data, 1))
+#define SHA512_COMPRESS(ctx, data) (_gnutls_padlock_sha512_blocks((void*)(ctx)->state, data, 1))
 
 void
 padlock_sha1_update(struct sha1_ctx *ctx,
@@ -339,7 +339,7 @@ int wrap_padlock_hash_fast(gnutls_digest_algorithm_t algo,
 			0x10325476UL,
 			0xC3D2E1F0UL,
 		};
-		padlock_sha1_oneshot(iv, text, text_size);
+		_gnutls_padlock_sha1_oneshot(iv, text, text_size);
 		_nettle_write_be32(20, digest, iv);
 	} else if (algo == GNUTLS_DIG_SHA256) {
 		uint32_t iv[8] = {
@@ -348,7 +348,7 @@ int wrap_padlock_hash_fast(gnutls_digest_algorithm_t algo,
 			0x510e527fUL, 0x9b05688cUL, 0x1f83d9abUL,
 			    0x5be0cd19UL,
 		};
-		padlock_sha256_oneshot(iv, text, text_size);
+		_gnutls_padlock_sha256_oneshot(iv, text, text_size);
 		_nettle_write_be32(32, digest, iv);
 	} else {
 		struct padlock_hash_ctx ctx;
diff --git a/lib/accelerated/x86/sha-padlock.h b/lib/accelerated/x86/sha-padlock.h
index af67a07..730cb74 100644
--- a/lib/accelerated/x86/sha-padlock.h
+++ b/lib/accelerated/x86/sha-padlock.h
@@ -3,14 +3,14 @@
 
 #include <nettle/sha.h>
 
-void padlock_sha1_oneshot(void *ctx, const void *inp, size_t len);
-void padlock_sha256_oneshot(void *ctx, const void *inp, size_t len);
+void _gnutls_padlock_sha1_oneshot(void *ctx, const void *inp, size_t len);
+void _gnutls_padlock_sha256_oneshot(void *ctx, const void *inp, size_t len);
 
-void padlock_sha1_blocks(unsigned int *ctx, const void *inp,
+void _gnutls_padlock_sha1_blocks(unsigned int *ctx, const void *inp,
 			 size_t blocks);
-void padlock_sha256_blocks(unsigned int *ctx, const void *inp,
+void _gnutls_padlock_sha256_blocks(unsigned int *ctx, const void *inp,
 			   size_t blocks);
-void padlock_sha512_blocks(unsigned int *ctx, const void *inp,
+void _gnutls_padlock_sha512_blocks(unsigned int *ctx, const void *inp,
 			   size_t blocks);
 
 int wrap_padlock_hash_fast(gnutls_digest_algorithm_t algo,
diff --git a/lib/accelerated/x86/sha-x86-ssse3.c b/lib/accelerated/x86/sha-x86-ssse3.c
index 8ea4e54..40d2e45 100644
--- a/lib/accelerated/x86/sha-x86-ssse3.c
+++ b/lib/accelerated/x86/sha-x86-ssse3.c
@@ -31,9 +31,9 @@
 #include <sha-x86.h>
 #include <x86-common.h>
 
-void sha1_block_data_order(void *c, const void *p, size_t len);
-void sha256_block_data_order(void *c, const void *p, size_t len);
-void sha512_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha1_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha256_block_data_order(void *c, const void *p, size_t len);
+void _gnutls_sha512_block_data_order(void *c, const void *p, size_t len);
 
 typedef void (*update_func) (void *, size_t, const uint8_t *);
 typedef void (*digest_func) (void *, size_t, uint8_t *);
@@ -108,7 +108,7 @@ void x86_sha1_update(struct sha1_ctx *ctx, size_t length,
 
 		t2 = length / SHA1_DATA_SIZE;
 
-		sha1_block_data_order(&octx, data, t2);
+		_gnutls_sha1_block_data_order(&octx, data, t2);
 
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -161,7 +161,7 @@ void x86_sha256_update(struct sha256_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA1_DATA_SIZE;
-		sha256_block_data_order(&octx, data, t2);
+		_gnutls_sha256_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			ctx->count++;
@@ -212,7 +212,7 @@ void x86_sha512_update(struct sha512_ctx *ctx, size_t length,
 
 	if (length > 0) {
 		t2 = length / SHA512_DATA_SIZE;
-		sha512_block_data_order(&octx, data, t2);
+		_gnutls_sha512_block_data_order(&octx, data, t2);
 		
 		for (i=0;i<t2;i++)
 			MD_INCR(ctx);
diff --git a/lib/accelerated/x86/x86-common.c b/lib/accelerated/x86/x86-common.c
index fb3ff90..69da338 100644
--- a/lib/accelerated/x86/x86-common.c
+++ b/lib/accelerated/x86/x86-common.c
@@ -295,8 +295,8 @@ static int check_phe_partial(void)
 		0x98BADCFEUL, 0x10325476UL, 0xC3D2E1F0UL
 	};
 
-	padlock_sha1_blocks(iv, text, sizeof(text) - 1);
-	padlock_sha1_blocks(iv, text, sizeof(text) - 1);
+	_gnutls_padlock_sha1_blocks(iv, text, sizeof(text) - 1);
+	_gnutls_padlock_sha1_blocks(iv, text, sizeof(text) - 1);
 
 	if (iv[0] == 0x9096E2D8UL && iv[1] == 0xA33074EEUL &&
 	    iv[2] == 0xCDBEE447UL && iv[3] == 0xEC7979D2UL &&
@@ -331,7 +331,7 @@ void register_x86_padlock_crypto(unsigned capabilities)
 		return;
 
 	if (capabilities == 0)
-		edx = padlock_capability();
+		edx = _gnutls_padlock_capability();
 	else
 		edx = capabilities_to_via_edx(capabilities);
 
-- 
2.23.0.rc1

